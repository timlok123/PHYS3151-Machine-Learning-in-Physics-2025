{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A module to implement the gradient descent learning algorithm for a feedforward neural network.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "    \n",
    "    # the list ''sizes'' contains the number of neurons in the respective layers of the network.\n",
    "    # [2, 3, 1] input layer 2 neurons, hidden layer 3 neurons, output layer 1 neuron\n",
    "    def __init__(self, sizes):\n",
    "\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "       \n",
    "    # return the output of the network if \"a\" is input.\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    # “training_data” is a list of tuples \"(x,y)\" representing the training inputs and the desired outputs.\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "            \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "                \n",
    "    # update the network's weights and biases by applying gradient descent \n",
    "    # using backpropagation to a single mini batch.\n",
    "    # The ''mini_batch'' is a list of tuples \"(x,y)\", and \"eta\" is the learning rate.\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                       for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
    "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) #* \\\n",
    "  #          sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # l=1 means the last layer of the neurons\n",
    "        # l=2 means the second-last layer\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    # the neural network's output is assumed to be the index of \n",
    "    # whichever neuron in the final layer has the highest activation.\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)),y)\n",
    "                       for (x,y) in test_data]\n",
    "        return sum(int(x == y) for (x,y) in test_results)\n",
    "    \n",
    "    # vector of partial derivatives \\partial C_X / \\partial a for the output activations.\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "    \n",
    "# Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A library to load the MNIST image data.\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
    "# The \"training_data\" is returned as a tuple with two entries.\n",
    "\n",
    "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is, \n",
    "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
    "\n",
    "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
    "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
    "\n",
    "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
    "def load_data():\n",
    "\n",
    "        f = gzip.open('mnist.pkl.gz','rb')\n",
    "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "        f.close()\n",
    "        return (training_data, validation_data, test_data)\n",
    "\n",
    "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
    "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\". \n",
    "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
    "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector \n",
    "# corresponding to the correct digit for \"x\".\n",
    "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
    "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
    "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
    "    \n",
    "    plt.imshow(training_inputs[9].reshape((28,28)),cmap=cm.Greys_r)\n",
    "    plt.show()\n",
    "    \n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "# Return a 10-dimensional unit vector with a 1.0 in the jth position and zeros elsewhere.\n",
    "# This is used to convert a digit (0...9) into a corresponding desired output from the neural network.\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10,1))\n",
    "    e[j] = 1.0\n",
    "    return e    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784,100,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 9656 / 10000\n",
      "Epoch 1 : 9661 / 10000\n",
      "Epoch 2 : 9662 / 10000\n",
      "Epoch 3 : 9664 / 10000\n",
      "Epoch 4 : 9667 / 10000\n",
      "Epoch 5 : 9667 / 10000\n",
      "Epoch 6 : 9667 / 10000\n",
      "Epoch 7 : 9668 / 10000\n",
      "Epoch 8 : 9667 / 10000\n",
      "Epoch 9 : 9669 / 10000\n",
      "Epoch 10 : 9668 / 10000\n",
      "Epoch 11 : 9668 / 10000\n",
      "Epoch 12 : 9668 / 10000\n",
      "Epoch 13 : 9669 / 10000\n",
      "Epoch 14 : 9668 / 10000\n",
      "Epoch 15 : 9669 / 10000\n",
      "Epoch 16 : 9668 / 10000\n",
      "Epoch 17 : 9667 / 10000\n",
      "Epoch 18 : 9670 / 10000\n",
      "Epoch 19 : 9670 / 10000\n",
      "Epoch 20 : 9669 / 10000\n",
      "Epoch 21 : 9668 / 10000\n",
      "Epoch 22 : 9669 / 10000\n",
      "Epoch 23 : 9669 / 10000\n",
      "Epoch 24 : 9670 / 10000\n",
      "Epoch 25 : 9670 / 10000\n",
      "Epoch 26 : 9669 / 10000\n",
      "Epoch 27 : 9669 / 10000\n",
      "Epoch 28 : 9671 / 10000\n",
      "Epoch 29 : 9671 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10000, 3.0, test_data=test_data)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
