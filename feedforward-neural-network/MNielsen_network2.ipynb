{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An improved version of MNielsen_network, implementing the SGD learning algorithm for a feedforward neural network.\n",
    "# Improvments include \n",
    "# 1. the cross-entropy cost function, \n",
    "# 2. regularization, \n",
    "# 3. better initialization of the weights.\n",
    "\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# define the quadratic and cross-entropy cost functions\n",
    "\n",
    "class QuadraticCost(object):\n",
    "    \n",
    "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "    \n",
    "    # return the error delta from the output layer.\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y)*sigmoid_prime(z)\n",
    "    \n",
    "class CrossEntropyCost(object):\n",
    "    \n",
    "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
    "    # np.nan_to_num is used to ensure numerical stability, make sure 0*log(0) = 0.0\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "    \n",
    "    # returm the error delta from the output layer.\n",
    "    # parameter \"z\" is not used by the method. \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y)\n",
    "    \n",
    "# Main Network class\n",
    "class Network2(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        \n",
    "        # the biases and weights for the network are initiated randomly, using \"self.default_weight_initializer\".\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost = cost\n",
    "    \n",
    "    def default_weight_initializer(self):\n",
    "        \n",
    "        # initialize each weight using a Gaussian distribution with mean 0 and standard deviation 1\n",
    "        # over the square root of the number of weights connecting to the same neuron. \n",
    "        # initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1.\n",
    "    \n",
    "        # for the input layers, there is no biases.\n",
    "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "            \n",
    "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    # \"evaluation_data\", monitor the cost and accuracy on either the evaluation data or the training data.\n",
    "    # returns a tuple containing four lists:\n",
    "    # 1. the (per-epoch) costs on the evaluation data,\n",
    "    # 2. the accuracies on the evaluation data,\n",
    "    # 3. the costs on the training data,\n",
    "    # 4. the accuracies on the training data.\n",
    "    # If we train for 30 epochs, then the first element of the tuple will be a \n",
    "    # 30-element list containing the cost on the evaluation data at the end of each epoch.\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda=0.0,\n",
    "            evaluation_data = None,\n",
    "            monitor_evaluation_cost = False,\n",
    "            monitor_evaluation_accuracy = False,\n",
    "            monitor_training_cost = False,\n",
    "            monitor_training_accuracy = False,\n",
    "            early_stopping_n = 0):\n",
    "        \n",
    "        # early stopping functionality:\n",
    "        best_accuracy=1\n",
    "        \n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        if evaluation_data:\n",
    "            evaluation_data = list(evaluation_data)\n",
    "            n_data = len(evaluation_data)\n",
    "            \n",
    "        # early stopping functionality:\n",
    "        best_accuracy=0\n",
    "        no_accuracy_change=0\n",
    "        \n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
    "                \n",
    "            print(\"Epoch %s training complete\" % j)\n",
    "            \n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print(\"Cost on training data: {}\".format(cost))\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on training data: {} / {}\".format(accuracy, n))\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(\"Cost on evaluation data: {}\".format(cost))\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
    "            \n",
    "            print()\n",
    "            \n",
    "            # Early stopping:\n",
    "            if early_stopping_n > 0:\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    no_accuracy_change = 0\n",
    "                    print(\"Early-stopping: Best so far{}\".format(best_accuracy))\n",
    "                else:\n",
    "                    no_accuracy_change += 1\n",
    "                    \n",
    "                if (no_accuracy_change == early_stopping_n):\n",
    "                    print(\"Early-stopping: No accuracy cahnge in last epochs: {}\".format(early_stopping_n))\n",
    "                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
    "        \n",
    "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
    "    \n",
    "    \n",
    "    # \"mini_batch\" is a list of tuples \"(x,y)\"\n",
    "    # \"eta\" is the learning rate\n",
    "    # \"lmbda\" is the regularization parameter\n",
    "    # \"n\" is the total size of the training set.\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        # \"eta*(lmbda/n)*w\" comes from the regularization term.\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
    "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        # backpropagate\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    \n",
    "    def accuracy(self, data, convert=False):\n",
    "     \n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
    "        \n",
    "        result_accuracy = sum(int(x==y) for (x,y) in results)\n",
    "        return result_accuracy\n",
    "    \n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "            cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "    \n",
    "    # Save the neural network to the file \"filename\".\n",
    "    def save(self, filename):\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "# Loading a Network\n",
    "def load(filename):\n",
    "    \n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "# Miscellaneous functions\n",
    "def vectorized_result(j):\n",
    "    \n",
    "    e = np.zeros((10,1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A library to load the MNIST image data.\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
    "# The \"training_data\" is returned as a tuple with two entries.\n",
    "\n",
    "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is, \n",
    "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
    "\n",
    "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
    "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
    "\n",
    "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
    "def load_data():\n",
    "\n",
    "        f = gzip.open('mnist.pkl.gz','rb')\n",
    "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "        f.close()\n",
    "        return (training_data, validation_data, test_data)\n",
    "\n",
    "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
    "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\". \n",
    "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
    "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector \n",
    "# corresponding to the correct digit for \"x\".\n",
    "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
    "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
    "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
    "    \n",
    "    plt.imshow(training_inputs[1].reshape((28,28)),cmap=cm.Greys_r)\n",
    "    plt.show()\n",
    "    \n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOH0lEQVR4nO3db4xVdX7H8c+3dEH5Y4IaCbpToWiMTU2hIWoyWAdXkPoEeGCzPKhsumF4sCaL6QN1a7Jq40hMd43GhDgbCbRuXTfiH7LW7jrDxlkTs2E0KrhTUCd0YUGIIeFPUBD49sEcmgHn/M5wz7n3XPi+X8nk3nu+c+755jIfzrn3d879mbsLwMXvz+puAEBrEHYgCMIOBEHYgSAIOxDEn7dyY2bGR/9Ak7m7jbW81J7dzJaY2Q4z+9TMHizzXACayxodZzezCZJ2SlokaY+krZJWuPsfEuuwZwearBl79pslferuw+5+QtIvJC0t8XwAmqhM2K+RtHvU4z3ZsrOYWbeZDZrZYIltASipzAd0Yx0qfOMw3d17JfVKHMYDdSqzZ98jqWPU429L2luuHQDNUibsWyVdb2azzWyipO9K2lxNWwCq1vBhvLufNLP7JP1a0gRJ693948o6A1CphofeGtoY79mBpmvKSTUALhyEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTR0imbcfHp6upK1h9++OHc2h133JFcd8uWLcn6Y489lqwPDAwk69GwZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIJjFFUmdnZ3Jel9fX7I+ceLEKts5y/Hjx5P1yZMnN23b7SxvFtdSJ9WY2S5JRySdknTS3eeXeT4AzVPFGXQL3f2LCp4HQBPxnh0IomzYXdJvzOw9M+se6xfMrNvMBs1ssOS2AJRQ9jC+0933mtlVkt4ys/9x97OuPnD3Xkm9Eh/QAXUqtWd3973Z7QFJr0q6uYqmAFSv4bCb2RQzm3bmvqTFkrZX1RiAapU5jJ8h6VUzO/M8/+nu/11JV2iZO++8M1nftGlTsj5p0qRkPXUex4kTJ5Lrnjp1Klm/9NJLk/UlS5bk1oqulS/q7ULUcNjdfVjS31TYC4AmYugNCIKwA0EQdiAIwg4EQdiBILjE9SIwZcqU3NrChQuT677wwgvJ+rRp05L1bOg1V+rva/fu3cl1e3p6kvV169Yl66nenn766eS6999/f7LezvIucWXPDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMGXzReCNN97Ird12220t7OT8dHR0JOtFY/w7d+5M1m+44Ybc2vz58b4ImT07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBOPsFoKurK1m/5ZZbcmtF15sX2bFjR7L+2muvJesPPPBAbu3o0aPJdd99991k/eDBg8n6+vXrc2tlX5cLEXt2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiC741vA52dncl6X19fsj5x4sSGt/3hhx8m67fffnuyvmzZsmR93rx5ubUnn3wyue7nn3+erBc5ffp0bu3rr79Orrto0aJkfWBgoKGeWqHh7403s/VmdsDMto9adrmZvWVmn2S306tsFkD1xnMYv0HSubPaPyip392vl9SfPQbQxgrD7u4Dks49L3GppI3Z/Y2S0sdyAGrX6LnxM9x9nyS5+z4zuyrvF82sW1J3g9sBUJGmXwjj7r2SeiU+oAPq1OjQ234zmylJ2e2B6loC0AyNhn2zpJXZ/ZWSXq+mHQDNUjjObmYvSuqSdKWk/ZJ+LOk1Sb+U9BeS/ijpHndPX1ysuIfxN910U7L+7LPPJutF3/1+7Nix3NqhQ4eS6z766KPJem9vb7LezlLj7EV/9++8806yXnT+QZ3yxtkL37O7+4qc0ndKdQSgpThdFgiCsANBEHYgCMIOBEHYgSD4KukKXHLJJcn6hg0bkvW5c+cm68ePH0/WV61alVvr7+9Prjt58uRkPaqrr7667hYqx54dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnL0CRVMqF42jF1mxIu/CwxFF0yYDEnt2IAzCDgRB2IEgCDsQBGEHgiDsQBCEHQiCKZsr8NlnnyXrs2fPTtZ37NiRrN94443n3RPSXxdd9Hc/PDycrF933XUN9dQKDU/ZDODiQNiBIAg7EARhB4Ig7EAQhB0IgrADQXA9+zjde++9ubWOjo7kukVjups2bWqoJ6SVGWfftm1b1e3UrnDPbmbrzeyAmW0ftewRM/uTmX2Q/dzd3DYBlDWew/gNkpaMsfwpd5+b/fxXtW0BqFph2N19QNLBFvQCoInKfEB3n5l9lB3mT8/7JTPrNrNBMxsssS0AJTUa9nWS5kiaK2mfpJ/k/aK797r7fHef3+C2AFSgobC7+353P+XupyX9TNLN1bYFoGoNhd3MZo56uFzS9rzfBdAeCsfZzexFSV2SrjSzPZJ+LKnLzOZKckm7JK1uYo9tITWP+YQJE5LrHjt2LFl/7rnnGurpYlc07/26desafu6hoaFkPXVexYWqMOzuPtYMBc83oRcATcTpskAQhB0IgrADQRB2IAjCDgTBJa4tcPLkyWR99+7dLeqkvRQNrT3zzDPJetHw2OHDh3Nrjz/+eHLdI0eOJOsXIvbsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE4+wt0NfXV3cLtens7Myt9fT0JNddsGBBsr5169Zk/dZbb03Wo2HPDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM4+TmbWUE2SFi1aVHU7beOJJ55I1tesWZNbmzRpUnLdt99+O1lfuHBhso6zsWcHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAYZx8nd2+oJklTp05N1l9++eVk/amnnkrW9+7dm1u76667kuuuWrUqWZ8zZ06yftlllyXrhw4dyq0NDg4m1127dm2yjvNTuGc3sw4z+62ZDZnZx2b2w2z55Wb2lpl9kt1Ob367ABo1nsP4k5L+2d1vlHSrpB+Y2V9JelBSv7tfL6k/ewygTRWG3d33ufv72f0jkoYkXSNpqaSN2a9tlLSsWU0CKO+83rOb2SxJ8yT9XtIMd98njfyHYGZX5azTLam7XJsAyhp32M1sqqRNkta4++Giiz/OcPdeSb3Zc6Q/yQLQNOMaejOzb2kk6D9391eyxfvNbGZWnynpQHNaBFCFwj27jezCn5c05O4/HVXaLGmlpLXZ7etN6fAiUHQUtHz58mR98eLFyfpXX32VW7viiiuS65Y1PDycrPf39+fWVq9eXXU7SBjPYXynpH+UtM3MPsiW/UgjIf+lmX1f0h8l3dOcFgFUoTDs7v6OpLxd03eqbQdAs3C6LBAEYQeCIOxAEIQdCIKwA0FY0eWZlW7sAj6DbtasWbm1LVu2JNe99tprS227aJy+zL/hl19+may/+eabyfo99zDi2m7cfcw/GPbsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE4+wV6OjoSNYfeuihZL3ouu4y4+wvvfRSct2enp5kffv27ck62g/j7EBwhB0IgrADQRB2IAjCDgRB2IEgCDsQBOPswEWGcXYgOMIOBEHYgSAIOxAEYQeCIOxAEIQdCKIw7GbWYWa/NbMhM/vYzH6YLX/EzP5kZh9kP3c3v10AjSo8qcbMZkqa6e7vm9k0Se9JWibpHyQddfd/G/fGOKkGaLq8k2rGMz/7Pkn7svtHzGxI0jXVtgeg2c7rPbuZzZI0T9Lvs0X3mdlHZrbezKbnrNNtZoNmNliqUwCljPvceDObKultSY+7+ytmNkPSF5Jc0r9q5FD/nwqeg8N4oMnyDuPHFXYz+5akX0n6tbv/dIz6LEm/cve/Lngewg40WcMXwtjIV5s+L2lodNCzD+7OWC6JryEF2th4Po1fIOl3krZJOp0t/pGkFZLmauQwfpek1dmHeannYs8ONFmpw/iqEHag+bieHQiOsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EEThF05W7AtJ/zvq8ZXZsnbUrr21a18SvTWqyt6uzSu09Hr2b2zcbNDd59fWQEK79taufUn01qhW9cZhPBAEYQeCqDvsvTVvP6Vde2vXviR6a1RLeqv1PTuA1ql7zw6gRQg7EEQtYTezJWa2w8w+NbMH6+ghj5ntMrNt2TTUtc5Pl82hd8DMto9adrmZvWVmn2S3Y86xV1NvbTGNd2Ka8Vpfu7qnP2/5e3YzmyBpp6RFkvZI2ipphbv/oaWN5DCzXZLmu3vtJ2CY2d9JOirp389MrWVmT0o66O5rs/8op7v7A23S2yM6z2m8m9Rb3jTj31ONr12V0583oo49+82SPnX3YXc/IekXkpbW0Efbc/cBSQfPWbxU0sbs/kaN/LG0XE5vbcHd97n7+9n9I5LOTDNe62uX6Ksl6gj7NZJ2j3q8R+0137tL+o2ZvWdm3XU3M4YZZ6bZym6vqrmfcxVO491K50wz3javXSPTn5dVR9jHmpqmncb/Ot39byX9vaQfZIerGJ91kuZoZA7AfZJ+Umcz2TTjmyStcffDdfYy2hh9teR1qyPseyR1jHr8bUl7a+hjTO6+N7s9IOlVjbztaCf7z8ygm90eqLmf/+fu+939lLuflvQz1fjaZdOMb5L0c3d/JVtc+2s3Vl+tet3qCPtWSdeb2Wwzmyjpu5I219DHN5jZlOyDE5nZFEmL1X5TUW+WtDK7v1LS6zX2cpZ2mcY7b5px1fza1T79ubu3/EfS3Rr5RP4zSf9SRw85ff2lpA+zn4/r7k3Sixo5rPtaI0dE35d0haR+SZ9kt5e3UW//oZGpvT/SSLBm1tTbAo28NfxI0gfZz911v3aJvlryunG6LBAEZ9ABQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBD/B0bJb6BnTJm2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network2([784, 30, 10], cost=CrossEntropyCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 3827.662107245836\n",
      "Accuracy on training data: 46922 / 50000\n",
      "Cost on evaluation data: 3827.654972744784\n",
      "Accuracy on evaluation data: 9384 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 4967.765571867481\n",
      "Accuracy on training data: 47012 / 50000\n",
      "Cost on evaluation data: 4967.773805158235\n",
      "Accuracy on evaluation data: 9415 / 10000\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 5538.102407348555\n",
      "Accuracy on training data: 47682 / 50000\n",
      "Cost on evaluation data: 5538.124248121075\n",
      "Accuracy on evaluation data: 9512 / 10000\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 5874.125826338354\n",
      "Accuracy on training data: 47949 / 50000\n",
      "Cost on evaluation data: 5874.150971685845\n",
      "Accuracy on evaluation data: 9545 / 10000\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 6095.720112365904\n",
      "Accuracy on training data: 48075 / 50000\n",
      "Cost on evaluation data: 6095.754712910357\n",
      "Accuracy on evaluation data: 9561 / 10000\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 6234.873716017939\n",
      "Accuracy on training data: 47819 / 50000\n",
      "Cost on evaluation data: 6234.89621872204\n",
      "Accuracy on evaluation data: 9539 / 10000\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 6299.783371604161\n",
      "Accuracy on training data: 48176 / 50000\n",
      "Cost on evaluation data: 6299.810125270498\n",
      "Accuracy on evaluation data: 9618 / 10000\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 6390.758465662764\n",
      "Accuracy on training data: 48263 / 50000\n",
      "Cost on evaluation data: 6390.796511398984\n",
      "Accuracy on evaluation data: 9581 / 10000\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 6422.796187058945\n",
      "Accuracy on training data: 48158 / 50000\n",
      "Cost on evaluation data: 6422.823753918947\n",
      "Accuracy on evaluation data: 9578 / 10000\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 6474.951957549149\n",
      "Accuracy on training data: 48311 / 50000\n",
      "Cost on evaluation data: 6474.994405714924\n",
      "Accuracy on evaluation data: 9600 / 10000\n",
      "\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 6482.772509974185\n",
      "Accuracy on training data: 48166 / 50000\n",
      "Cost on evaluation data: 6482.808583609542\n",
      "Accuracy on evaluation data: 9574 / 10000\n",
      "\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 6524.708722403302\n",
      "Accuracy on training data: 48025 / 50000\n",
      "Cost on evaluation data: 6524.741903662258\n",
      "Accuracy on evaluation data: 9571 / 10000\n",
      "\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 6549.12340011799\n",
      "Accuracy on training data: 47946 / 50000\n",
      "Cost on evaluation data: 6549.156790914078\n",
      "Accuracy on evaluation data: 9544 / 10000\n",
      "\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 6553.413623785559\n",
      "Accuracy on training data: 48220 / 50000\n",
      "Cost on evaluation data: 6553.453465428393\n",
      "Accuracy on evaluation data: 9573 / 10000\n",
      "\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 6555.983246479955\n",
      "Accuracy on training data: 48459 / 50000\n",
      "Cost on evaluation data: 6556.016133262081\n",
      "Accuracy on evaluation data: 9631 / 10000\n",
      "\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 6626.291801982533\n",
      "Accuracy on training data: 48321 / 50000\n",
      "Cost on evaluation data: 6626.334824549119\n",
      "Accuracy on evaluation data: 9600 / 10000\n",
      "\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 6610.529004503693\n",
      "Accuracy on training data: 48050 / 50000\n",
      "Cost on evaluation data: 6610.570604952888\n",
      "Accuracy on evaluation data: 9553 / 10000\n",
      "\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 6632.849054036854\n",
      "Accuracy on training data: 48260 / 50000\n",
      "Cost on evaluation data: 6632.877282226619\n",
      "Accuracy on evaluation data: 9592 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 6635.577331062886\n",
      "Accuracy on training data: 48486 / 50000\n",
      "Cost on evaluation data: 6635.619627696006\n",
      "Accuracy on evaluation data: 9607 / 10000\n",
      "\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 6690.4932881895875\n",
      "Accuracy on training data: 48428 / 50000\n",
      "Cost on evaluation data: 6690.540689888126\n",
      "Accuracy on evaluation data: 9589 / 10000\n",
      "\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 6647.8942785502995\n",
      "Accuracy on training data: 48387 / 50000\n",
      "Cost on evaluation data: 6647.94041969377\n",
      "Accuracy on evaluation data: 9598 / 10000\n",
      "\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 6660.273530608465\n",
      "Accuracy on training data: 48432 / 50000\n",
      "Cost on evaluation data: 6660.318779831006\n",
      "Accuracy on evaluation data: 9605 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 6646.581870303596\n",
      "Accuracy on training data: 48382 / 50000\n",
      "Cost on evaluation data: 6646.624424960405\n",
      "Accuracy on evaluation data: 9591 / 10000\n",
      "\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 6675.937202865037\n",
      "Accuracy on training data: 48399 / 50000\n",
      "Cost on evaluation data: 6675.983289225715\n",
      "Accuracy on evaluation data: 9599 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 6662.647585573006\n",
      "Accuracy on training data: 48319 / 50000\n",
      "Cost on evaluation data: 6662.685875321902\n",
      "Accuracy on evaluation data: 9603 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 6676.760285920601\n",
      "Accuracy on training data: 48255 / 50000\n",
      "Cost on evaluation data: 6676.788065128633\n",
      "Accuracy on evaluation data: 9600 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 6703.788296131702\n",
      "Accuracy on training data: 48411 / 50000\n",
      "Cost on evaluation data: 6703.8262773331835\n",
      "Accuracy on evaluation data: 9606 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 6742.316018811798\n",
      "Accuracy on training data: 48425 / 50000\n",
      "Cost on evaluation data: 6742.363059980183\n",
      "Accuracy on evaluation data: 9610 / 10000\n",
      "\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 6727.624213774382\n",
      "Accuracy on training data: 48460 / 50000\n",
      "Cost on evaluation data: 6727.66232128075\n",
      "Accuracy on evaluation data: 9615 / 10000\n",
      "\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 6719.023153823239\n",
      "Accuracy on training data: 48275 / 50000\n",
      "Cost on evaluation data: 6719.071572497623\n",
      "Accuracy on evaluation data: 9562 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([3827.654972744784,\n",
       "  4967.773805158235,\n",
       "  5538.124248121075,\n",
       "  5874.150971685845,\n",
       "  6095.754712910357,\n",
       "  6234.89621872204,\n",
       "  6299.810125270498,\n",
       "  6390.796511398984,\n",
       "  6422.823753918947,\n",
       "  6474.994405714924,\n",
       "  6482.808583609542,\n",
       "  6524.741903662258,\n",
       "  6549.156790914078,\n",
       "  6553.453465428393,\n",
       "  6556.016133262081,\n",
       "  6626.334824549119,\n",
       "  6610.570604952888,\n",
       "  6632.877282226619,\n",
       "  6635.619627696006,\n",
       "  6690.540689888126,\n",
       "  6647.94041969377,\n",
       "  6660.318779831006,\n",
       "  6646.624424960405,\n",
       "  6675.983289225715,\n",
       "  6662.685875321902,\n",
       "  6676.788065128633,\n",
       "  6703.8262773331835,\n",
       "  6742.363059980183,\n",
       "  6727.66232128075,\n",
       "  6719.071572497623],\n",
       " [9384,\n",
       "  9415,\n",
       "  9512,\n",
       "  9545,\n",
       "  9561,\n",
       "  9539,\n",
       "  9618,\n",
       "  9581,\n",
       "  9578,\n",
       "  9600,\n",
       "  9574,\n",
       "  9571,\n",
       "  9544,\n",
       "  9573,\n",
       "  9631,\n",
       "  9600,\n",
       "  9553,\n",
       "  9592,\n",
       "  9607,\n",
       "  9589,\n",
       "  9598,\n",
       "  9605,\n",
       "  9591,\n",
       "  9599,\n",
       "  9603,\n",
       "  9600,\n",
       "  9606,\n",
       "  9610,\n",
       "  9615,\n",
       "  9562],\n",
       " [3827.662107245836,\n",
       "  4967.765571867481,\n",
       "  5538.102407348555,\n",
       "  5874.125826338354,\n",
       "  6095.720112365904,\n",
       "  6234.873716017939,\n",
       "  6299.783371604161,\n",
       "  6390.758465662764,\n",
       "  6422.796187058945,\n",
       "  6474.951957549149,\n",
       "  6482.772509974185,\n",
       "  6524.708722403302,\n",
       "  6549.12340011799,\n",
       "  6553.413623785559,\n",
       "  6555.983246479955,\n",
       "  6626.291801982533,\n",
       "  6610.529004503693,\n",
       "  6632.849054036854,\n",
       "  6635.577331062886,\n",
       "  6690.4932881895875,\n",
       "  6647.8942785502995,\n",
       "  6660.273530608465,\n",
       "  6646.581870303596,\n",
       "  6675.937202865037,\n",
       "  6662.647585573006,\n",
       "  6676.760285920601,\n",
       "  6703.788296131702,\n",
       "  6742.316018811798,\n",
       "  6727.624213774382,\n",
       "  6719.023153823239],\n",
       " [46922,\n",
       "  47012,\n",
       "  47682,\n",
       "  47949,\n",
       "  48075,\n",
       "  47819,\n",
       "  48176,\n",
       "  48263,\n",
       "  48158,\n",
       "  48311,\n",
       "  48166,\n",
       "  48025,\n",
       "  47946,\n",
       "  48220,\n",
       "  48459,\n",
       "  48321,\n",
       "  48050,\n",
       "  48260,\n",
       "  48486,\n",
       "  48428,\n",
       "  48387,\n",
       "  48432,\n",
       "  48382,\n",
       "  48399,\n",
       "  48319,\n",
       "  48255,\n",
       "  48411,\n",
       "  48425,\n",
       "  48460,\n",
       "  48275])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
    "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "netQuadratic = Network2([784, 30, 10], cost=QuadraticCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 983.7314812494101\n",
      "Accuracy on training data: 45761 / 50000\n",
      "Cost on evaluation data: 983.72344184882\n",
      "Accuracy on evaluation data: 9243 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1187.7994988584387\n",
      "Accuracy on training data: 46500 / 50000\n",
      "Cost on evaluation data: 1187.7944466338574\n",
      "Accuracy on evaluation data: 9359 / 10000\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1291.5260954411178\n",
      "Accuracy on training data: 46868 / 50000\n",
      "Cost on evaluation data: 1291.5213435501032\n",
      "Accuracy on evaluation data: 9431 / 10000\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1349.1887245367486\n",
      "Accuracy on training data: 47086 / 50000\n",
      "Cost on evaluation data: 1349.1852261197196\n",
      "Accuracy on evaluation data: 9451 / 10000\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1390.2472006410928\n",
      "Accuracy on training data: 47220 / 50000\n",
      "Cost on evaluation data: 1390.2439292629012\n",
      "Accuracy on evaluation data: 9488 / 10000\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1418.5769920904363\n",
      "Accuracy on training data: 47357 / 50000\n",
      "Cost on evaluation data: 1418.5741826405447\n",
      "Accuracy on evaluation data: 9504 / 10000\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1436.1175590198413\n",
      "Accuracy on training data: 47450 / 50000\n",
      "Cost on evaluation data: 1436.1148700123758\n",
      "Accuracy on evaluation data: 9538 / 10000\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1453.8539013665322\n",
      "Accuracy on training data: 47374 / 50000\n",
      "Cost on evaluation data: 1453.85102897469\n",
      "Accuracy on evaluation data: 9507 / 10000\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1462.0347918476216\n",
      "Accuracy on training data: 47521 / 50000\n",
      "Cost on evaluation data: 1462.0323966786798\n",
      "Accuracy on evaluation data: 9536 / 10000\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1472.7254693421535\n",
      "Accuracy on training data: 47529 / 50000\n",
      "Cost on evaluation data: 1472.7231097190913\n",
      "Accuracy on evaluation data: 9561 / 10000\n",
      "\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 1478.7906400599124\n",
      "Accuracy on training data: 47531 / 50000\n",
      "Cost on evaluation data: 1478.7886158996707\n",
      "Accuracy on evaluation data: 9541 / 10000\n",
      "\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 1483.3361221825564\n",
      "Accuracy on training data: 47587 / 50000\n",
      "Cost on evaluation data: 1483.333912146917\n",
      "Accuracy on evaluation data: 9554 / 10000\n",
      "\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 1489.9426855151885\n",
      "Accuracy on training data: 47586 / 50000\n",
      "Cost on evaluation data: 1489.9406298913611\n",
      "Accuracy on evaluation data: 9551 / 10000\n",
      "\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 1492.8229892041277\n",
      "Accuracy on training data: 47532 / 50000\n",
      "Cost on evaluation data: 1492.8207661244787\n",
      "Accuracy on evaluation data: 9547 / 10000\n",
      "\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 1496.3491902024987\n",
      "Accuracy on training data: 47623 / 50000\n",
      "Cost on evaluation data: 1496.3469731280372\n",
      "Accuracy on evaluation data: 9569 / 10000\n",
      "\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 1499.9095846317962\n",
      "Accuracy on training data: 47677 / 50000\n",
      "Cost on evaluation data: 1499.9077062639117\n",
      "Accuracy on evaluation data: 9575 / 10000\n",
      "\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 1504.427364057116\n",
      "Accuracy on training data: 47689 / 50000\n",
      "Cost on evaluation data: 1504.4255480105908\n",
      "Accuracy on evaluation data: 9566 / 10000\n",
      "\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 1504.6414247223795\n",
      "Accuracy on training data: 47718 / 50000\n",
      "Cost on evaluation data: 1504.6392863895987\n",
      "Accuracy on evaluation data: 9583 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 1506.4321763668706\n",
      "Accuracy on training data: 47717 / 50000\n",
      "Cost on evaluation data: 1506.4301203489288\n",
      "Accuracy on evaluation data: 9575 / 10000\n",
      "\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 1508.6386294555841\n",
      "Accuracy on training data: 47708 / 50000\n",
      "Cost on evaluation data: 1508.6366769809558\n",
      "Accuracy on evaluation data: 9572 / 10000\n",
      "\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 1510.7341786174275\n",
      "Accuracy on training data: 47747 / 50000\n",
      "Cost on evaluation data: 1510.7322497715263\n",
      "Accuracy on evaluation data: 9592 / 10000\n",
      "\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 1511.6315179959468\n",
      "Accuracy on training data: 47691 / 50000\n",
      "Cost on evaluation data: 1511.629626709609\n",
      "Accuracy on evaluation data: 9562 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 1511.3013982232767\n",
      "Accuracy on training data: 47700 / 50000\n",
      "Cost on evaluation data: 1511.2997809997732\n",
      "Accuracy on evaluation data: 9568 / 10000\n",
      "\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 1517.446019045782\n",
      "Accuracy on training data: 47735 / 50000\n",
      "Cost on evaluation data: 1517.4445115552303\n",
      "Accuracy on evaluation data: 9571 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 1516.7030372445254\n",
      "Accuracy on training data: 47723 / 50000\n",
      "Cost on evaluation data: 1516.7013562454042\n",
      "Accuracy on evaluation data: 9564 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 1519.112693840321\n",
      "Accuracy on training data: 47801 / 50000\n",
      "Cost on evaluation data: 1519.1112917258424\n",
      "Accuracy on evaluation data: 9584 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 1520.8228948175217\n",
      "Accuracy on training data: 47831 / 50000\n",
      "Cost on evaluation data: 1520.821344104862\n",
      "Accuracy on evaluation data: 9586 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 1523.973575105976\n",
      "Accuracy on training data: 47749 / 50000\n",
      "Cost on evaluation data: 1523.9719431798947\n",
      "Accuracy on evaluation data: 9578 / 10000\n",
      "\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 1525.5444273407002\n",
      "Accuracy on training data: 47742 / 50000\n",
      "Cost on evaluation data: 1525.542676705834\n",
      "Accuracy on evaluation data: 9590 / 10000\n",
      "\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 1525.8055173618625\n",
      "Accuracy on training data: 47804 / 50000\n",
      "Cost on evaluation data: 1525.8041770741138\n",
      "Accuracy on evaluation data: 9576 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([983.72344184882,\n",
       "  1187.7944466338574,\n",
       "  1291.5213435501032,\n",
       "  1349.1852261197196,\n",
       "  1390.2439292629012,\n",
       "  1418.5741826405447,\n",
       "  1436.1148700123758,\n",
       "  1453.85102897469,\n",
       "  1462.0323966786798,\n",
       "  1472.7231097190913,\n",
       "  1478.7886158996707,\n",
       "  1483.333912146917,\n",
       "  1489.9406298913611,\n",
       "  1492.8207661244787,\n",
       "  1496.3469731280372,\n",
       "  1499.9077062639117,\n",
       "  1504.4255480105908,\n",
       "  1504.6392863895987,\n",
       "  1506.4301203489288,\n",
       "  1508.6366769809558,\n",
       "  1510.7322497715263,\n",
       "  1511.629626709609,\n",
       "  1511.2997809997732,\n",
       "  1517.4445115552303,\n",
       "  1516.7013562454042,\n",
       "  1519.1112917258424,\n",
       "  1520.821344104862,\n",
       "  1523.9719431798947,\n",
       "  1525.542676705834,\n",
       "  1525.8041770741138],\n",
       " [9243,\n",
       "  9359,\n",
       "  9431,\n",
       "  9451,\n",
       "  9488,\n",
       "  9504,\n",
       "  9538,\n",
       "  9507,\n",
       "  9536,\n",
       "  9561,\n",
       "  9541,\n",
       "  9554,\n",
       "  9551,\n",
       "  9547,\n",
       "  9569,\n",
       "  9575,\n",
       "  9566,\n",
       "  9583,\n",
       "  9575,\n",
       "  9572,\n",
       "  9592,\n",
       "  9562,\n",
       "  9568,\n",
       "  9571,\n",
       "  9564,\n",
       "  9584,\n",
       "  9586,\n",
       "  9578,\n",
       "  9590,\n",
       "  9576],\n",
       " [983.7314812494101,\n",
       "  1187.7994988584387,\n",
       "  1291.5260954411178,\n",
       "  1349.1887245367486,\n",
       "  1390.2472006410928,\n",
       "  1418.5769920904363,\n",
       "  1436.1175590198413,\n",
       "  1453.8539013665322,\n",
       "  1462.0347918476216,\n",
       "  1472.7254693421535,\n",
       "  1478.7906400599124,\n",
       "  1483.3361221825564,\n",
       "  1489.9426855151885,\n",
       "  1492.8229892041277,\n",
       "  1496.3491902024987,\n",
       "  1499.9095846317962,\n",
       "  1504.427364057116,\n",
       "  1504.6414247223795,\n",
       "  1506.4321763668706,\n",
       "  1508.6386294555841,\n",
       "  1510.7341786174275,\n",
       "  1511.6315179959468,\n",
       "  1511.3013982232767,\n",
       "  1517.446019045782,\n",
       "  1516.7030372445254,\n",
       "  1519.112693840321,\n",
       "  1520.8228948175217,\n",
       "  1523.973575105976,\n",
       "  1525.5444273407002,\n",
       "  1525.8055173618625],\n",
       " [45761,\n",
       "  46500,\n",
       "  46868,\n",
       "  47086,\n",
       "  47220,\n",
       "  47357,\n",
       "  47450,\n",
       "  47374,\n",
       "  47521,\n",
       "  47529,\n",
       "  47531,\n",
       "  47587,\n",
       "  47586,\n",
       "  47532,\n",
       "  47623,\n",
       "  47677,\n",
       "  47689,\n",
       "  47718,\n",
       "  47717,\n",
       "  47708,\n",
       "  47747,\n",
       "  47691,\n",
       "  47700,\n",
       "  47735,\n",
       "  47723,\n",
       "  47801,\n",
       "  47831,\n",
       "  47749,\n",
       "  47742,\n",
       "  47804])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netQuadratic.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
    "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
