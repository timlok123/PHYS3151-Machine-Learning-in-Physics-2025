{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timlok123/PHYS3151-Machine-Learning-in-Physics-2025/blob/main/feedforward-neural-network/MNielsen_network_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dXoM7ZwcSdGv"
      },
      "outputs": [],
      "source": [
        "# A module to implement the gradient descent learning algorithm for a feedforward neural network.\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "\n",
        "    # the list ''sizes'' contains the number of neurons in the respective layers of the network.\n",
        "    # [2, 3, 1] input layer 2 neurons, hidden layer 3 neurons, output layer 1 neuron\n",
        "    def __init__(self, sizes):\n",
        "\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    # return the output of the network if \"a\" is input.\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    # “training_data” is a list of tuples \"(x,y)\" representing the training inputs and the desired outputs.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
        "            else:\n",
        "                print(\"Epoch {} complete\".format(j))\n",
        "\n",
        "    # update the network's weights and biases by applying gradient descent\n",
        "    # using backpropagation to a single mini batch,i.e., for each mini_batch we apply a single step of gradient descent.\n",
        "    # Therefore, to have sufficient update of the weights, it is good to have smaller mini_batch_size.\n",
        "    # The ''mini_batch'' is a list of tuples \"(x,y)\", and \"eta\" is the learning rate.\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                       for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # l=1 means the last layer of the neurons\n",
        "        # l=2 means the second-last layer\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    # the neural network's output is assumed to be the index of\n",
        "    # whichever neuron in the final layer has the highest activation.\n",
        "    def evaluate(self, test_data):\n",
        "        test_results = [(np.argmax(self.feedforward(x)),y)\n",
        "                       for (x,y) in test_data]\n",
        "        return sum(int(x == y) for (x,y) in test_results)\n",
        "\n",
        "    # vector of partial derivatives \\partial C_X / \\partial a for the output activations.\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "\n",
        "# Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Therefore, to have sufficient update of the weights, it is good to have smaller mini_batch_size**."
      ],
      "metadata": {
        "id": "oWvHo56pn1oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neural_network = Network([2,3,2,1])\n",
        "# show the random biases\n",
        "neural_network.biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg7xJiLiBUSl",
        "outputId": "961e3d5f-daa9-4e96-97c7-a24482a97cad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.32818828],\n",
              "        [-1.19195032],\n",
              "        [ 0.21624452]]),\n",
              " array([[ 1.11648717],\n",
              "        [-0.74020323]]),\n",
              " array([[1.70144367]])]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show the random weights\n",
        "neural_network.weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYzcbJs2CMQ-",
        "outputId": "c79423d6-0f2f-4d70-bd49-9ca0f9d776cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.44189885, -1.89078766],\n",
              "        [ 0.086733  ,  1.54341195],\n",
              "        [-1.49795096,  0.71258518]]),\n",
              " array([[ 1.40977945, -1.6497521 ,  0.0497926 ],\n",
              "        [ 0.59003282, -1.67723559,  2.51827127]]),\n",
              " array([[-0.05618096,  0.03689151]])]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/timlok123/PHYS3151-Machine-Learning-in-Physics-2025.git"
      ],
      "metadata": {
        "id": "RoEQB1tkTO4i",
        "outputId": "88a6f18f-edd2-4f7b-ba2c-e7496f063882",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'PHYS3151-Machine-Learning-in-Physics-2025' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9bt3SXZUSdGy"
      },
      "outputs": [],
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is,\n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2025/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\".\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector\n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "\n",
        "    plt.imshow(training_inputs[4999].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a 10-dimensional unit vector with a 1.0 in the jth position and zeros elsewhere.\n",
        "# This is used to convert a digit (0...9) into a corresponding desired output from the neural network.\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5k-jTJDrSdGy",
        "outputId": "2f07f435-60c3-4b07-fdc7-da91b7ab521f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqQPTo_mSdGz"
      },
      "outputs": [],
      "source": [
        "net = Network([784,30,20,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2frayMZSdGz",
        "outputId": "25b45374-cc40-4e85-b6fc-0f70b2d2b9a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 : 9081 / 10000\n",
            "Epoch 1 : 9246 / 10000\n",
            "Epoch 2 : 9256 / 10000\n",
            "Epoch 3 : 9336 / 10000\n",
            "Epoch 4 : 9406 / 10000\n",
            "Epoch 5 : 9410 / 10000\n",
            "Epoch 6 : 9462 / 10000\n",
            "Epoch 7 : 9403 / 10000\n",
            "Epoch 8 : 9435 / 10000\n",
            "Epoch 9 : 9434 / 10000\n",
            "Epoch 10 : 9460 / 10000\n",
            "Epoch 11 : 9471 / 10000\n",
            "Epoch 12 : 9477 / 10000\n",
            "Epoch 13 : 9487 / 10000\n",
            "Epoch 14 : 9493 / 10000\n",
            "Epoch 15 : 9485 / 10000\n",
            "Epoch 16 : 9494 / 10000\n",
            "Epoch 17 : 9518 / 10000\n",
            "Epoch 18 : 9482 / 10000\n",
            "Epoch 19 : 9520 / 10000\n",
            "Epoch 20 : 9508 / 10000\n",
            "Epoch 21 : 9524 / 10000\n",
            "Epoch 22 : 9504 / 10000\n",
            "Epoch 23 : 9476 / 10000\n",
            "Epoch 24 : 9548 / 10000\n",
            "Epoch 25 : 9516 / 10000\n",
            "Epoch 26 : 9511 / 10000\n",
            "Epoch 27 : 9528 / 10000\n",
            "Epoch 28 : 9533 / 10000\n",
            "Epoch 29 : 9520 / 10000\n"
          ]
        }
      ],
      "source": [
        "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LRs0ll3ISdGz"
      },
      "outputs": [],
      "source": [
        "# An improved version of MNielsen_network, implementing the SGD learning algorithm for a feedforward neural network.\n",
        "# Improvments include\n",
        "# 1. the cross-entropy cost function,\n",
        "# 2. regularization,\n",
        "# 3. better initialization of the weights.\n",
        "\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# define the quadratic and cross-entropy cost functions\n",
        "\n",
        "class QuadraticCost(object):\n",
        "\n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return 0.5*np.linalg.norm(a-y)**2\n",
        "\n",
        "    # return the error delta from the output layer.\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)*sigmoid_prime(z)\n",
        "\n",
        "class CrossEntropyCost(object):\n",
        "\n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    # np.nan_to_num is used to ensure numerical stability, make sure 0*log(0) = 0.0\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "\n",
        "    # returm the error delta from the output layer.\n",
        "    # parameter \"z\" is not used by the method.\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)\n",
        "\n",
        "# Main Network class\n",
        "class Network2(object):\n",
        "\n",
        "\n",
        "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "\n",
        "        # the biases and weights for the network are initiated randomly, using \"self.default_weight_initializer\".\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.default_weight_initializer()\n",
        "        self.cost = cost\n",
        "\n",
        "    def default_weight_initializer(self):\n",
        "\n",
        "        # initialize each weight using a Gaussian distribution with mean 0 and standard deviation 1\n",
        "        # over the square root of the number of weights connecting to the same neuron.\n",
        "        # initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "\n",
        "        # for the input layers, there is no biases.\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def large_weight_initializer(self):\n",
        "\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    # \"evaluation_data\", monitor the cost and accuracy on either the evaluation data or the training data.\n",
        "    # returns a tuple containing four lists:\n",
        "    # 1. the (per-epoch) costs on the evaluation data,\n",
        "    # 2. the accuracies on the evaluation data,\n",
        "    # 3. the costs on the training data,\n",
        "    # 4. the accuracies on the training data.\n",
        "    # If we train for 30 epochs, then the first element of the tuple will be a\n",
        "    # 30-element list containing the cost on the evaluation data at the end of each epoch.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            lmbda=0.0,\n",
        "            evaluation_data = None,\n",
        "            monitor_evaluation_cost = False,\n",
        "            monitor_evaluation_accuracy = False,\n",
        "            monitor_training_cost = False,\n",
        "            monitor_training_accuracy = False,\n",
        "            early_stopping_n = 0):\n",
        "\n",
        "        # early stopping functionality:\n",
        "        best_accuracy=1\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if evaluation_data:\n",
        "            evaluation_data = list(evaluation_data)\n",
        "            n_data = len(evaluation_data)\n",
        "\n",
        "        # early stopping functionality:\n",
        "        best_accuracy=0\n",
        "        no_accuracy_change=0\n",
        "\n",
        "        evaluation_cost, evaluation_accuracy = [], []\n",
        "        training_cost, training_accuracy = [], []\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "\n",
        "            print(\"Epoch %s training complete\" % j)\n",
        "\n",
        "            if monitor_training_cost:\n",
        "                cost = self.total_cost(training_data, lmbda)\n",
        "                training_cost.append(cost)\n",
        "                print(\"Cost on training data: {}\".format(cost))\n",
        "            if monitor_training_accuracy:\n",
        "                accuracy = self.accuracy(training_data, convert=True)\n",
        "                training_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on training data: {} / {}\".format(accuracy, n))\n",
        "            if monitor_evaluation_cost:\n",
        "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                evaluation_cost.append(cost)\n",
        "                print(\"Cost on evaluation data: {}\".format(cost))\n",
        "            if monitor_evaluation_accuracy:\n",
        "                accuracy = self.accuracy(evaluation_data)\n",
        "                evaluation_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
        "\n",
        "            print()\n",
        "\n",
        "            # Early stopping:\n",
        "            if early_stopping_n > 0:\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    no_accuracy_change = 0\n",
        "                    print(\"Early-stopping: Best so far{}\".format(best_accuracy))\n",
        "                else:\n",
        "                    no_accuracy_change += 1\n",
        "\n",
        "                if (no_accuracy_change == early_stopping_n):\n",
        "                    print(\"Early-stopping: No accuracy cahnge in last epochs: {}\".format(early_stopping_n))\n",
        "                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "\n",
        "    # \"mini_batch\" is a list of tuples \"(x,y)\"\n",
        "    # \"eta\" is the learning rate\n",
        "    # \"lmbda\" is the regularization parameter\n",
        "    # \"n\" is the total size of the training set.\n",
        "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        # \"eta*(lmbda/n)*w\" comes from the regularization term.\n",
        "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # backpropagate\n",
        "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "\n",
        "    def accuracy(self, data, convert=False):\n",
        "\n",
        "        if convert:\n",
        "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
        "        else:\n",
        "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
        "\n",
        "        result_accuracy = sum(int(x==y) for (x,y) in results)\n",
        "        return result_accuracy\n",
        "\n",
        "    def total_cost(self, data, lmbda, convert=False):\n",
        "        cost = 0.0\n",
        "        for x, y in data:\n",
        "            a = self.feedforward(x)\n",
        "            if convert: y = vectorized_result(y)\n",
        "            cost += self.cost.fn(a, y)/len(data)\n",
        "            cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
        "        return cost\n",
        "\n",
        "    # Save the neural network to the file \"filename\".\n",
        "    def save(self, filename):\n",
        "        data = {\"sizes\": self.sizes,\n",
        "                \"weights\": [w.tolist() for w in self.weights],\n",
        "                \"biases\": [b.tolist() for b in self.biases],\n",
        "                \"cost\": str(self.cost.__name__)}\n",
        "        f = open(filename, \"w\")\n",
        "        json.dump(data, f)\n",
        "        f.close()\n",
        "\n",
        "\n",
        "# Loading a Network\n",
        "def load(filename):\n",
        "\n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "\n",
        "# Miscellaneous functions\n",
        "def vectorized_result(j):\n",
        "\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is,\n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2025/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\".\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector\n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "\n",
        "    plt.imshow(training_inputs[10022].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)"
      ],
      "metadata": {
        "id": "4KCSryRc-XA5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ],
      "metadata": {
        "id": "czAce4vR-Y_-",
        "outputId": "1a8d3913-ffb3-4fc8-a95b-1e7d2a170c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = Network2([784, 30, 20, 10], cost=CrossEntropyCost)\n",
        "net2.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)"
      ],
      "metadata": {
        "id": "m3vhgcz_-bEl",
        "outputId": "b1809955-0477-4f4c-fdcf-f4694339de71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 5117.242689450198\n",
            "Accuracy on training data: 46550 / 50000\n",
            "Cost on evaluation data: 5117.227760376865\n",
            "Accuracy on evaluation data: 9384 / 10000\n",
            "\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 6544.567019978626\n",
            "Accuracy on training data: 47503 / 50000\n",
            "Cost on evaluation data: 6544.567615452682\n",
            "Accuracy on evaluation data: 9493 / 10000\n",
            "\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 7182.3759387924665\n",
            "Accuracy on training data: 47674 / 50000\n",
            "Cost on evaluation data: 7182.390376760451\n",
            "Accuracy on evaluation data: 9516 / 10000\n",
            "\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 7504.965669825484\n",
            "Accuracy on training data: 47809 / 50000\n",
            "Cost on evaluation data: 7504.987180776239\n",
            "Accuracy on evaluation data: 9534 / 10000\n",
            "\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 7764.888142887838\n",
            "Accuracy on training data: 47502 / 50000\n",
            "Cost on evaluation data: 7764.910944340737\n",
            "Accuracy on evaluation data: 9473 / 10000\n",
            "\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 7903.989464694979\n",
            "Accuracy on training data: 47589 / 50000\n",
            "Cost on evaluation data: 7904.008760617409\n",
            "Accuracy on evaluation data: 9499 / 10000\n",
            "\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 8042.884760132196\n",
            "Accuracy on training data: 48131 / 50000\n",
            "Cost on evaluation data: 8042.92458873886\n",
            "Accuracy on evaluation data: 9582 / 10000\n",
            "\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 8156.747541146365\n",
            "Accuracy on training data: 47744 / 50000\n",
            "Cost on evaluation data: 8156.7837804201\n",
            "Accuracy on evaluation data: 9484 / 10000\n",
            "\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 8208.273723766295\n",
            "Accuracy on training data: 48151 / 50000\n",
            "Cost on evaluation data: 8208.314940499533\n",
            "Accuracy on evaluation data: 9568 / 10000\n",
            "\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 8231.280068426939\n",
            "Accuracy on training data: 47951 / 50000\n",
            "Cost on evaluation data: 8231.309719493445\n",
            "Accuracy on evaluation data: 9539 / 10000\n",
            "\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 8177.646833516656\n",
            "Accuracy on training data: 48440 / 50000\n",
            "Cost on evaluation data: 8177.687793615424\n",
            "Accuracy on evaluation data: 9633 / 10000\n",
            "\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 8224.358084035932\n",
            "Accuracy on training data: 48067 / 50000\n",
            "Cost on evaluation data: 8224.389807553427\n",
            "Accuracy on evaluation data: 9551 / 10000\n",
            "\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 8288.068554556856\n",
            "Accuracy on training data: 48228 / 50000\n",
            "Cost on evaluation data: 8288.091889091462\n",
            "Accuracy on evaluation data: 9616 / 10000\n",
            "\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 8319.132813423379\n",
            "Accuracy on training data: 47970 / 50000\n",
            "Cost on evaluation data: 8319.166319151214\n",
            "Accuracy on evaluation data: 9554 / 10000\n",
            "\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 8284.376652421133\n",
            "Accuracy on training data: 48265 / 50000\n",
            "Cost on evaluation data: 8284.412565604916\n",
            "Accuracy on evaluation data: 9587 / 10000\n",
            "\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 8298.492207014458\n",
            "Accuracy on training data: 47593 / 50000\n",
            "Cost on evaluation data: 8298.524829510758\n",
            "Accuracy on evaluation data: 9488 / 10000\n",
            "\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 8347.019984809263\n",
            "Accuracy on training data: 48215 / 50000\n",
            "Cost on evaluation data: 8347.046256219926\n",
            "Accuracy on evaluation data: 9587 / 10000\n",
            "\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 8349.668271827142\n",
            "Accuracy on training data: 47952 / 50000\n",
            "Cost on evaluation data: 8349.696071331271\n",
            "Accuracy on evaluation data: 9573 / 10000\n",
            "\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 8323.668396124425\n",
            "Accuracy on training data: 48089 / 50000\n",
            "Cost on evaluation data: 8323.698922360196\n",
            "Accuracy on evaluation data: 9580 / 10000\n",
            "\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 8340.190492241189\n",
            "Accuracy on training data: 48166 / 50000\n",
            "Cost on evaluation data: 8340.230199263435\n",
            "Accuracy on evaluation data: 9570 / 10000\n",
            "\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 8379.988737955733\n",
            "Accuracy on training data: 48501 / 50000\n",
            "Cost on evaluation data: 8380.029535337997\n",
            "Accuracy on evaluation data: 9621 / 10000\n",
            "\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 8342.731717623385\n",
            "Accuracy on training data: 47478 / 50000\n",
            "Cost on evaluation data: 8342.751085622307\n",
            "Accuracy on evaluation data: 9475 / 10000\n",
            "\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 8330.432013287505\n",
            "Accuracy on training data: 48403 / 50000\n",
            "Cost on evaluation data: 8330.463381763788\n",
            "Accuracy on evaluation data: 9615 / 10000\n",
            "\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 8437.346543310943\n",
            "Accuracy on training data: 48473 / 50000\n",
            "Cost on evaluation data: 8437.38149918704\n",
            "Accuracy on evaluation data: 9648 / 10000\n",
            "\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 8400.600985446143\n",
            "Accuracy on training data: 48184 / 50000\n",
            "Cost on evaluation data: 8400.642527394546\n",
            "Accuracy on evaluation data: 9579 / 10000\n",
            "\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 8452.412895610683\n",
            "Accuracy on training data: 48096 / 50000\n",
            "Cost on evaluation data: 8452.443068083041\n",
            "Accuracy on evaluation data: 9567 / 10000\n",
            "\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 8384.420651675839\n",
            "Accuracy on training data: 48168 / 50000\n",
            "Cost on evaluation data: 8384.457907014837\n",
            "Accuracy on evaluation data: 9572 / 10000\n",
            "\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 8390.446412538995\n",
            "Accuracy on training data: 48449 / 50000\n",
            "Cost on evaluation data: 8390.476107074934\n",
            "Accuracy on evaluation data: 9632 / 10000\n",
            "\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 8369.516731896296\n",
            "Accuracy on training data: 48307 / 50000\n",
            "Cost on evaluation data: 8369.559047340119\n",
            "Accuracy on evaluation data: 9583 / 10000\n",
            "\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 8407.8800023647\n",
            "Accuracy on training data: 48408 / 50000\n",
            "Cost on evaluation data: 8407.913804458543\n",
            "Accuracy on evaluation data: 9636 / 10000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([np.float64(5117.227760376865),\n",
              "  np.float64(6544.567615452682),\n",
              "  np.float64(7182.390376760451),\n",
              "  np.float64(7504.987180776239),\n",
              "  np.float64(7764.910944340737),\n",
              "  np.float64(7904.008760617409),\n",
              "  np.float64(8042.92458873886),\n",
              "  np.float64(8156.7837804201),\n",
              "  np.float64(8208.314940499533),\n",
              "  np.float64(8231.309719493445),\n",
              "  np.float64(8177.687793615424),\n",
              "  np.float64(8224.389807553427),\n",
              "  np.float64(8288.091889091462),\n",
              "  np.float64(8319.166319151214),\n",
              "  np.float64(8284.412565604916),\n",
              "  np.float64(8298.524829510758),\n",
              "  np.float64(8347.046256219926),\n",
              "  np.float64(8349.696071331271),\n",
              "  np.float64(8323.698922360196),\n",
              "  np.float64(8340.230199263435),\n",
              "  np.float64(8380.029535337997),\n",
              "  np.float64(8342.751085622307),\n",
              "  np.float64(8330.463381763788),\n",
              "  np.float64(8437.38149918704),\n",
              "  np.float64(8400.642527394546),\n",
              "  np.float64(8452.443068083041),\n",
              "  np.float64(8384.457907014837),\n",
              "  np.float64(8390.476107074934),\n",
              "  np.float64(8369.559047340119),\n",
              "  np.float64(8407.913804458543)],\n",
              " [9384,\n",
              "  9493,\n",
              "  9516,\n",
              "  9534,\n",
              "  9473,\n",
              "  9499,\n",
              "  9582,\n",
              "  9484,\n",
              "  9568,\n",
              "  9539,\n",
              "  9633,\n",
              "  9551,\n",
              "  9616,\n",
              "  9554,\n",
              "  9587,\n",
              "  9488,\n",
              "  9587,\n",
              "  9573,\n",
              "  9580,\n",
              "  9570,\n",
              "  9621,\n",
              "  9475,\n",
              "  9615,\n",
              "  9648,\n",
              "  9579,\n",
              "  9567,\n",
              "  9572,\n",
              "  9632,\n",
              "  9583,\n",
              "  9636],\n",
              " [np.float64(5117.242689450198),\n",
              "  np.float64(6544.567019978626),\n",
              "  np.float64(7182.3759387924665),\n",
              "  np.float64(7504.965669825484),\n",
              "  np.float64(7764.888142887838),\n",
              "  np.float64(7903.989464694979),\n",
              "  np.float64(8042.884760132196),\n",
              "  np.float64(8156.747541146365),\n",
              "  np.float64(8208.273723766295),\n",
              "  np.float64(8231.280068426939),\n",
              "  np.float64(8177.646833516656),\n",
              "  np.float64(8224.358084035932),\n",
              "  np.float64(8288.068554556856),\n",
              "  np.float64(8319.132813423379),\n",
              "  np.float64(8284.376652421133),\n",
              "  np.float64(8298.492207014458),\n",
              "  np.float64(8347.019984809263),\n",
              "  np.float64(8349.668271827142),\n",
              "  np.float64(8323.668396124425),\n",
              "  np.float64(8340.190492241189),\n",
              "  np.float64(8379.988737955733),\n",
              "  np.float64(8342.731717623385),\n",
              "  np.float64(8330.432013287505),\n",
              "  np.float64(8437.346543310943),\n",
              "  np.float64(8400.600985446143),\n",
              "  np.float64(8452.412895610683),\n",
              "  np.float64(8384.420651675839),\n",
              "  np.float64(8390.446412538995),\n",
              "  np.float64(8369.516731896296),\n",
              "  np.float64(8407.8800023647)],\n",
              " [46550,\n",
              "  47503,\n",
              "  47674,\n",
              "  47809,\n",
              "  47502,\n",
              "  47589,\n",
              "  48131,\n",
              "  47744,\n",
              "  48151,\n",
              "  47951,\n",
              "  48440,\n",
              "  48067,\n",
              "  48228,\n",
              "  47970,\n",
              "  48265,\n",
              "  47593,\n",
              "  48215,\n",
              "  47952,\n",
              "  48089,\n",
              "  48166,\n",
              "  48501,\n",
              "  47478,\n",
              "  48403,\n",
              "  48473,\n",
              "  48184,\n",
              "  48096,\n",
              "  48168,\n",
              "  48449,\n",
              "  48307,\n",
              "  48408])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()\n",
        "netQuadratic = Network2([784, 30,20,10], cost=QuadraticCost)\n",
        "netQuadratic.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)\n"
      ],
      "metadata": {
        "id": "JlPtN5Wa-fed",
        "outputId": "cfd0ca94-724f-45f0-9a9c-3e9d0484a76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 1404.6578766017963\n",
            "Accuracy on training data: 45102 / 50000\n",
            "Cost on evaluation data: 1404.650600794143\n",
            "Accuracy on evaluation data: 9085 / 10000\n",
            "\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 1742.5788453016517\n",
            "Accuracy on training data: 46176 / 50000\n",
            "Cost on evaluation data: 1742.5736935667053\n",
            "Accuracy on evaluation data: 9282 / 10000\n",
            "\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 1870.3078121210767\n",
            "Accuracy on training data: 46910 / 50000\n",
            "Cost on evaluation data: 1870.30438433811\n",
            "Accuracy on evaluation data: 9420 / 10000\n",
            "\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 1938.9517857332262\n",
            "Accuracy on training data: 47179 / 50000\n",
            "Cost on evaluation data: 1938.9486742673816\n",
            "Accuracy on evaluation data: 9472 / 10000\n",
            "\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 1985.5000505478224\n",
            "Accuracy on training data: 47292 / 50000\n",
            "Cost on evaluation data: 1985.498403524812\n",
            "Accuracy on evaluation data: 9467 / 10000\n",
            "\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 2019.0430458002243\n",
            "Accuracy on training data: 47537 / 50000\n",
            "Cost on evaluation data: 2019.0409184145988\n",
            "Accuracy on evaluation data: 9524 / 10000\n",
            "\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 2038.4134625186741\n",
            "Accuracy on training data: 47529 / 50000\n",
            "Cost on evaluation data: 2038.4115156936252\n",
            "Accuracy on evaluation data: 9528 / 10000\n",
            "\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 2057.2762868630307\n",
            "Accuracy on training data: 47728 / 50000\n",
            "Cost on evaluation data: 2057.2750077265514\n",
            "Accuracy on evaluation data: 9558 / 10000\n",
            "\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 2071.743694247027\n",
            "Accuracy on training data: 47791 / 50000\n",
            "Cost on evaluation data: 2071.7423103388096\n",
            "Accuracy on evaluation data: 9575 / 10000\n",
            "\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 2081.545349483038\n",
            "Accuracy on training data: 47670 / 50000\n",
            "Cost on evaluation data: 2081.5442460540776\n",
            "Accuracy on evaluation data: 9552 / 10000\n",
            "\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 2084.4500316454687\n",
            "Accuracy on training data: 47911 / 50000\n",
            "Cost on evaluation data: 2084.4491100919554\n",
            "Accuracy on evaluation data: 9580 / 10000\n",
            "\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 2094.99917732301\n",
            "Accuracy on training data: 47966 / 50000\n",
            "Cost on evaluation data: 2094.9984585872708\n",
            "Accuracy on evaluation data: 9583 / 10000\n",
            "\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 2099.699663550135\n",
            "Accuracy on training data: 47903 / 50000\n",
            "Cost on evaluation data: 2099.698538081786\n",
            "Accuracy on evaluation data: 9577 / 10000\n",
            "\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 2103.270136809089\n",
            "Accuracy on training data: 47981 / 50000\n",
            "Cost on evaluation data: 2103.2692039203266\n",
            "Accuracy on evaluation data: 9610 / 10000\n",
            "\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 2105.2942848618086\n",
            "Accuracy on training data: 47890 / 50000\n",
            "Cost on evaluation data: 2105.293893994648\n",
            "Accuracy on evaluation data: 9583 / 10000\n",
            "\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 2105.2726976743043\n",
            "Accuracy on training data: 47854 / 50000\n",
            "Cost on evaluation data: 2105.2722692217058\n",
            "Accuracy on evaluation data: 9560 / 10000\n",
            "\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 2107.8259561004\n",
            "Accuracy on training data: 48024 / 50000\n",
            "Cost on evaluation data: 2107.825333455769\n",
            "Accuracy on evaluation data: 9599 / 10000\n",
            "\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 2113.115311617435\n",
            "Accuracy on training data: 48044 / 50000\n",
            "Cost on evaluation data: 2113.1151449392974\n",
            "Accuracy on evaluation data: 9604 / 10000\n",
            "\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 2116.344167192127\n",
            "Accuracy on training data: 48109 / 50000\n",
            "Cost on evaluation data: 2116.343975477142\n",
            "Accuracy on evaluation data: 9616 / 10000\n",
            "\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 2119.521907666666\n",
            "Accuracy on training data: 48088 / 50000\n",
            "Cost on evaluation data: 2119.521290361718\n",
            "Accuracy on evaluation data: 9615 / 10000\n",
            "\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 2121.295588384019\n",
            "Accuracy on training data: 48108 / 50000\n",
            "Cost on evaluation data: 2121.295186987756\n",
            "Accuracy on evaluation data: 9621 / 10000\n",
            "\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 2122.893361825139\n",
            "Accuracy on training data: 48159 / 50000\n",
            "Cost on evaluation data: 2122.8929739986024\n",
            "Accuracy on evaluation data: 9610 / 10000\n",
            "\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 2120.250704732642\n",
            "Accuracy on training data: 48107 / 50000\n",
            "Cost on evaluation data: 2120.2501566138585\n",
            "Accuracy on evaluation data: 9603 / 10000\n",
            "\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 2120.8668729458536\n",
            "Accuracy on training data: 48096 / 50000\n",
            "Cost on evaluation data: 2120.8668746503013\n",
            "Accuracy on evaluation data: 9609 / 10000\n",
            "\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 2122.52784786888\n",
            "Accuracy on training data: 48157 / 50000\n",
            "Cost on evaluation data: 2122.5272064639375\n",
            "Accuracy on evaluation data: 9621 / 10000\n",
            "\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 2126.5509819419985\n",
            "Accuracy on training data: 48207 / 50000\n",
            "Cost on evaluation data: 2126.550876144581\n",
            "Accuracy on evaluation data: 9631 / 10000\n",
            "\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 2125.4408662838414\n",
            "Accuracy on training data: 48205 / 50000\n",
            "Cost on evaluation data: 2125.4405966842146\n",
            "Accuracy on evaluation data: 9634 / 10000\n",
            "\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 2125.8745322281356\n",
            "Accuracy on training data: 48079 / 50000\n",
            "Cost on evaluation data: 2125.8747859740374\n",
            "Accuracy on evaluation data: 9595 / 10000\n",
            "\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 2130.08539381121\n",
            "Accuracy on training data: 48224 / 50000\n",
            "Cost on evaluation data: 2130.084964022559\n",
            "Accuracy on evaluation data: 9633 / 10000\n",
            "\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 2129.3600682343495\n",
            "Accuracy on training data: 48177 / 50000\n",
            "Cost on evaluation data: 2129.3599714973366\n",
            "Accuracy on evaluation data: 9614 / 10000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([np.float64(1404.650600794143),\n",
              "  np.float64(1742.5736935667053),\n",
              "  np.float64(1870.30438433811),\n",
              "  np.float64(1938.9486742673816),\n",
              "  np.float64(1985.498403524812),\n",
              "  np.float64(2019.0409184145988),\n",
              "  np.float64(2038.4115156936252),\n",
              "  np.float64(2057.2750077265514),\n",
              "  np.float64(2071.7423103388096),\n",
              "  np.float64(2081.5442460540776),\n",
              "  np.float64(2084.4491100919554),\n",
              "  np.float64(2094.9984585872708),\n",
              "  np.float64(2099.698538081786),\n",
              "  np.float64(2103.2692039203266),\n",
              "  np.float64(2105.293893994648),\n",
              "  np.float64(2105.2722692217058),\n",
              "  np.float64(2107.825333455769),\n",
              "  np.float64(2113.1151449392974),\n",
              "  np.float64(2116.343975477142),\n",
              "  np.float64(2119.521290361718),\n",
              "  np.float64(2121.295186987756),\n",
              "  np.float64(2122.8929739986024),\n",
              "  np.float64(2120.2501566138585),\n",
              "  np.float64(2120.8668746503013),\n",
              "  np.float64(2122.5272064639375),\n",
              "  np.float64(2126.550876144581),\n",
              "  np.float64(2125.4405966842146),\n",
              "  np.float64(2125.8747859740374),\n",
              "  np.float64(2130.084964022559),\n",
              "  np.float64(2129.3599714973366)],\n",
              " [9085,\n",
              "  9282,\n",
              "  9420,\n",
              "  9472,\n",
              "  9467,\n",
              "  9524,\n",
              "  9528,\n",
              "  9558,\n",
              "  9575,\n",
              "  9552,\n",
              "  9580,\n",
              "  9583,\n",
              "  9577,\n",
              "  9610,\n",
              "  9583,\n",
              "  9560,\n",
              "  9599,\n",
              "  9604,\n",
              "  9616,\n",
              "  9615,\n",
              "  9621,\n",
              "  9610,\n",
              "  9603,\n",
              "  9609,\n",
              "  9621,\n",
              "  9631,\n",
              "  9634,\n",
              "  9595,\n",
              "  9633,\n",
              "  9614],\n",
              " [np.float64(1404.6578766017963),\n",
              "  np.float64(1742.5788453016517),\n",
              "  np.float64(1870.3078121210767),\n",
              "  np.float64(1938.9517857332262),\n",
              "  np.float64(1985.5000505478224),\n",
              "  np.float64(2019.0430458002243),\n",
              "  np.float64(2038.4134625186741),\n",
              "  np.float64(2057.2762868630307),\n",
              "  np.float64(2071.743694247027),\n",
              "  np.float64(2081.545349483038),\n",
              "  np.float64(2084.4500316454687),\n",
              "  np.float64(2094.99917732301),\n",
              "  np.float64(2099.699663550135),\n",
              "  np.float64(2103.270136809089),\n",
              "  np.float64(2105.2942848618086),\n",
              "  np.float64(2105.2726976743043),\n",
              "  np.float64(2107.8259561004),\n",
              "  np.float64(2113.115311617435),\n",
              "  np.float64(2116.344167192127),\n",
              "  np.float64(2119.521907666666),\n",
              "  np.float64(2121.295588384019),\n",
              "  np.float64(2122.893361825139),\n",
              "  np.float64(2120.250704732642),\n",
              "  np.float64(2120.8668729458536),\n",
              "  np.float64(2122.52784786888),\n",
              "  np.float64(2126.5509819419985),\n",
              "  np.float64(2125.4408662838414),\n",
              "  np.float64(2125.8745322281356),\n",
              "  np.float64(2130.08539381121),\n",
              "  np.float64(2129.3600682343495)],\n",
              " [45102,\n",
              "  46176,\n",
              "  46910,\n",
              "  47179,\n",
              "  47292,\n",
              "  47537,\n",
              "  47529,\n",
              "  47728,\n",
              "  47791,\n",
              "  47670,\n",
              "  47911,\n",
              "  47966,\n",
              "  47903,\n",
              "  47981,\n",
              "  47890,\n",
              "  47854,\n",
              "  48024,\n",
              "  48044,\n",
              "  48109,\n",
              "  48088,\n",
              "  48108,\n",
              "  48159,\n",
              "  48107,\n",
              "  48096,\n",
              "  48157,\n",
              "  48207,\n",
              "  48205,\n",
              "  48079,\n",
              "  48224,\n",
              "  48177])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CempKnnysdJq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "MNielsen_network.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}