{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timlok123/PHYS3151-Machine-Learning-in-Physics-2025/blob/main/feedforward-neural-network/MNielsen_network_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dXoM7ZwcSdGv"
      },
      "outputs": [],
      "source": [
        "# A module to implement the gradient descent learning algorithm for a feedforward neural network.\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "\n",
        "    # the list ''sizes'' contains the number of neurons in the respective layers of the network.\n",
        "    # [2, 3, 1] input layer 2 neurons, hidden layer 3 neurons, output layer 1 neuron\n",
        "    def __init__(self, sizes):\n",
        "\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    # return the output of the network if \"a\" is input.\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    # “training_data” is a list of tuples \"(x,y)\" representing the training inputs and the desired outputs.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
        "            else:\n",
        "                print(\"Epoch {} complete\".format(j))\n",
        "\n",
        "    # update the network's weights and biases by applying gradient descent\n",
        "    # using backpropagation to a single mini batch,i.e., for each mini_batch we apply a single step of gradient descent.\n",
        "    # Therefore, to have sufficient update of the weights, it is good to have smaller mini_batch_size.\n",
        "    # The ''mini_batch'' is a list of tuples \"(x,y)\", and \"eta\" is the learning rate.\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                       for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # l=1 means the last layer of the neurons\n",
        "        # l=2 means the second-last layer\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    # the neural network's output is assumed to be the index of\n",
        "    # whichever neuron in the final layer has the highest activation.\n",
        "    def evaluate(self, test_data):\n",
        "        test_results = [(np.argmax(self.feedforward(x)),y)\n",
        "                       for (x,y) in test_data]\n",
        "        return sum(int(x == y) for (x,y) in test_results)\n",
        "\n",
        "    # vector of partial derivatives \\partial C_X / \\partial a for the output activations.\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "\n",
        "# Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Therefore, to have sufficient update of the weights, it is good to have smaller mini_batch_size**."
      ],
      "metadata": {
        "id": "oWvHo56pn1oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neural_network = Network([2,3,2,1])\n",
        "# show the random biases\n",
        "neural_network.biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg7xJiLiBUSl",
        "outputId": "62aa7725-d268-4d97-a505-658f59a38bda"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.09822817],\n",
              "        [-1.02846399],\n",
              "        [ 0.18357016]]),\n",
              " array([[-1.51841211],\n",
              "        [-0.88869112]]),\n",
              " array([[-0.33741172]])]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show the random weights\n",
        "neural_network.weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYzcbJs2CMQ-",
        "outputId": "8992b255-10c4-4170-e0df-7edd5b2dd826"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.23295002, -0.40991864],\n",
              "        [-0.46066756,  1.01176381],\n",
              "        [ 0.77662843, -0.28637183]]),\n",
              " array([[-0.24103752,  0.28749145, -0.17183883],\n",
              "        [-0.1258246 ,  1.55921662, -1.6523474 ]]),\n",
              " array([[0.89517422, 1.68471442]])]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/timlok123/PHYS3151-Machine-Learning-in-Physics-2025.git"
      ],
      "metadata": {
        "id": "RoEQB1tkTO4i",
        "outputId": "853772be-bf8d-46f8-f14c-b7e7a990c3f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PHYS3151-Machine-Learning-in-Physics-2025'...\n",
            "remote: Enumerating objects: 269, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 269 (delta 14), reused 5 (delta 2), pack-reused 239 (from 1)\u001b[K\n",
            "Receiving objects: 100% (269/269), 47.26 MiB | 11.20 MiB/s, done.\n",
            "Resolving deltas: 100% (121/121), done.\n",
            "Updating files: 100% (62/62), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9bt3SXZUSdGy"
      },
      "outputs": [],
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is,\n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2025/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\".\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector\n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "\n",
        "    plt.imshow(training_inputs[4999].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a 10-dimensional unit vector with a 1.0 in the jth position and zeros elsewhere.\n",
        "# This is used to convert a digit (0...9) into a corresponding desired output from the neural network.\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5k-jTJDrSdGy",
        "outputId": "616f5da7-89a0-45d8-e9c1-05a0974bf725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iqQPTo_mSdGz"
      },
      "outputs": [],
      "source": [
        "net = Network([784,30,20,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k2frayMZSdGz",
        "outputId": "25b45374-cc40-4e85-b6fc-0f70b2d2b9a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 : 9081 / 10000\n",
            "Epoch 1 : 9246 / 10000\n",
            "Epoch 2 : 9256 / 10000\n",
            "Epoch 3 : 9336 / 10000\n",
            "Epoch 4 : 9406 / 10000\n",
            "Epoch 5 : 9410 / 10000\n",
            "Epoch 6 : 9462 / 10000\n",
            "Epoch 7 : 9403 / 10000\n",
            "Epoch 8 : 9435 / 10000\n",
            "Epoch 9 : 9434 / 10000\n",
            "Epoch 10 : 9460 / 10000\n",
            "Epoch 11 : 9471 / 10000\n",
            "Epoch 12 : 9477 / 10000\n",
            "Epoch 13 : 9487 / 10000\n",
            "Epoch 14 : 9493 / 10000\n",
            "Epoch 15 : 9485 / 10000\n",
            "Epoch 16 : 9494 / 10000\n",
            "Epoch 17 : 9518 / 10000\n",
            "Epoch 18 : 9482 / 10000\n",
            "Epoch 19 : 9520 / 10000\n",
            "Epoch 20 : 9508 / 10000\n",
            "Epoch 21 : 9524 / 10000\n",
            "Epoch 22 : 9504 / 10000\n",
            "Epoch 23 : 9476 / 10000\n",
            "Epoch 24 : 9548 / 10000\n",
            "Epoch 25 : 9516 / 10000\n",
            "Epoch 26 : 9511 / 10000\n",
            "Epoch 27 : 9528 / 10000\n",
            "Epoch 28 : 9533 / 10000\n",
            "Epoch 29 : 9520 / 10000\n"
          ]
        }
      ],
      "source": [
        "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRs0ll3ISdGz"
      },
      "outputs": [],
      "source": [
        "# An improved version of MNielsen_network, implementing the SGD learning algorithm for a feedforward neural network.\n",
        "# Improvments include\n",
        "# 1. the cross-entropy cost function,\n",
        "# 2. regularization,\n",
        "# 3. better initialization of the weights.\n",
        "\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# define the quadratic and cross-entropy cost functions\n",
        "\n",
        "class QuadraticCost(object):\n",
        "\n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return 0.5*np.linalg.norm(a-y)**2\n",
        "\n",
        "    # return the error delta from the output layer.\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)*sigmoid_prime(z)\n",
        "\n",
        "class CrossEntropyCost(object):\n",
        "\n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    # np.nan_to_num is used to ensure numerical stability, make sure 0*log(0) = 0.0\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "\n",
        "    # returm the error delta from the output layer.\n",
        "    # parameter \"z\" is not used by the method.\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)\n",
        "\n",
        "# Main Network class\n",
        "class Network2(object):\n",
        "\n",
        "\n",
        "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "\n",
        "        # the biases and weights for the network are initiated randomly, using \"self.default_weight_initializer\".\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.default_weight_initializer()\n",
        "        self.cost = cost\n",
        "\n",
        "    def default_weight_initializer(self):\n",
        "\n",
        "        # initialize each weight using a Gaussian distribution with mean 0 and standard deviation 1\n",
        "        # over the square root of the number of weights connecting to the same neuron.\n",
        "        # initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "\n",
        "        # for the input layers, there is no biases.\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def large_weight_initializer(self):\n",
        "\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    # \"evaluation_data\", monitor the cost and accuracy on either the evaluation data or the training data.\n",
        "    # returns a tuple containing four lists:\n",
        "    # 1. the (per-epoch) costs on the evaluation data,\n",
        "    # 2. the accuracies on the evaluation data,\n",
        "    # 3. the costs on the training data,\n",
        "    # 4. the accuracies on the training data.\n",
        "    # If we train for 30 epochs, then the first element of the tuple will be a\n",
        "    # 30-element list containing the cost on the evaluation data at the end of each epoch.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            lmbda=0.0,\n",
        "            evaluation_data = None,\n",
        "            monitor_evaluation_cost = False,\n",
        "            monitor_evaluation_accuracy = False,\n",
        "            monitor_training_cost = False,\n",
        "            monitor_training_accuracy = False,\n",
        "            early_stopping_n = 0):\n",
        "\n",
        "        # early stopping functionality:\n",
        "        best_accuracy=1\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if evaluation_data:\n",
        "            evaluation_data = list(evaluation_data)\n",
        "            n_data = len(evaluation_data)\n",
        "\n",
        "        # early stopping functionality:\n",
        "        best_accuracy=0\n",
        "        no_accuracy_change=0\n",
        "\n",
        "        evaluation_cost, evaluation_accuracy = [], []\n",
        "        training_cost, training_accuracy = [], []\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "\n",
        "            print(\"Epoch %s training complete\" % j)\n",
        "\n",
        "            if monitor_training_cost:\n",
        "                cost = self.total_cost(training_data, lmbda)\n",
        "                training_cost.append(cost)\n",
        "                print(\"Cost on training data: {}\".format(cost))\n",
        "            if monitor_training_accuracy:\n",
        "                accuracy = self.accuracy(training_data, convert=True)\n",
        "                training_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on training data: {} / {}\".format(accuracy, n))\n",
        "            if monitor_evaluation_cost:\n",
        "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                evaluation_cost.append(cost)\n",
        "                print(\"Cost on evaluation data: {}\".format(cost))\n",
        "            if monitor_evaluation_accuracy:\n",
        "                accuracy = self.accuracy(evaluation_data)\n",
        "                evaluation_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
        "\n",
        "            print()\n",
        "\n",
        "            # Early stopping:\n",
        "            if early_stopping_n > 0:\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    no_accuracy_change = 0\n",
        "                    print(\"Early-stopping: Best so far{}\".format(best_accuracy))\n",
        "                else:\n",
        "                    no_accuracy_change += 1\n",
        "\n",
        "                if (no_accuracy_change == early_stopping_n):\n",
        "                    print(\"Early-stopping: No accuracy cahnge in last epochs: {}\".format(early_stopping_n))\n",
        "                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "\n",
        "    # \"mini_batch\" is a list of tuples \"(x,y)\"\n",
        "    # \"eta\" is the learning rate\n",
        "    # \"lmbda\" is the regularization parameter\n",
        "    # \"n\" is the total size of the training set.\n",
        "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        # \"eta*(lmbda/n)*w\" comes from the regularization term.\n",
        "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # backpropagate\n",
        "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "\n",
        "    def accuracy(self, data, convert=False):\n",
        "\n",
        "        if convert:\n",
        "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
        "        else:\n",
        "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
        "\n",
        "        result_accuracy = sum(int(x==y) for (x,y) in results)\n",
        "        return result_accuracy\n",
        "\n",
        "    def total_cost(self, data, lmbda, convert=False):\n",
        "        cost = 0.0\n",
        "        for x, y in data:\n",
        "            a = self.feedforward(x)\n",
        "            if convert: y = vectorized_result(y)\n",
        "            cost += self.cost.fn(a, y)/len(data)\n",
        "            cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
        "        return cost\n",
        "\n",
        "    # Save the neural network to the file \"filename\".\n",
        "    def save(self, filename):\n",
        "        data = {\"sizes\": self.sizes,\n",
        "                \"weights\": [w.tolist() for w in self.weights],\n",
        "                \"biases\": [b.tolist() for b in self.biases],\n",
        "                \"cost\": str(self.cost.__name__)}\n",
        "        f = open(filename, \"w\")\n",
        "        json.dump(data, f)\n",
        "        f.close()\n",
        "\n",
        "\n",
        "# Loading a Network\n",
        "def load(filename):\n",
        "\n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "\n",
        "# Miscellaneous functions\n",
        "def vectorized_result(j):\n",
        "\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is,\n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2025/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\".\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector\n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "\n",
        "    plt.imshow(training_inputs[1].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)"
      ],
      "metadata": {
        "id": "4KCSryRc-XA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ],
      "metadata": {
        "id": "czAce4vR-Y_-",
        "outputId": "828fc2d0-39df-4a50-c5e5-cd1d14964935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = Network2([784, 30, 10], cost=CrossEntropyCost)\n",
        "net2.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)"
      ],
      "metadata": {
        "id": "m3vhgcz_-bEl",
        "outputId": "1b519a75-eddc-42b6-e276-6095285f58a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 3831.925901892078\n",
            "Accuracy on training data: 46640 / 50000\n",
            "Cost on evaluation data: 3831.9193873439735\n",
            "Accuracy on evaluation data: 9349 / 10000\n",
            "\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 4914.662161375608\n",
            "Accuracy on training data: 47445 / 50000\n",
            "Cost on evaluation data: 4914.682078440934\n",
            "Accuracy on evaluation data: 9477 / 10000\n",
            "\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 5522.388878729542\n",
            "Accuracy on training data: 47501 / 50000\n",
            "Cost on evaluation data: 5522.40779384175\n",
            "Accuracy on evaluation data: 9455 / 10000\n",
            "\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 5815.879008859292\n",
            "Accuracy on training data: 47877 / 50000\n",
            "Cost on evaluation data: 5815.904707025302\n",
            "Accuracy on evaluation data: 9538 / 10000\n",
            "\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 5999.1194948603215\n",
            "Accuracy on training data: 48086 / 50000\n",
            "Cost on evaluation data: 5999.153797915321\n",
            "Accuracy on evaluation data: 9579 / 10000\n",
            "\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 6131.930495662223\n",
            "Accuracy on training data: 48013 / 50000\n",
            "Cost on evaluation data: 6131.958856703971\n",
            "Accuracy on evaluation data: 9566 / 10000\n",
            "\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 6240.08630934203\n",
            "Accuracy on training data: 48118 / 50000\n",
            "Cost on evaluation data: 6240.129024886401\n",
            "Accuracy on evaluation data: 9587 / 10000\n",
            "\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 6306.128389551453\n",
            "Accuracy on training data: 48151 / 50000\n",
            "Cost on evaluation data: 6306.160530853341\n",
            "Accuracy on evaluation data: 9562 / 10000\n",
            "\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 6327.475040966342\n",
            "Accuracy on training data: 47726 / 50000\n",
            "Cost on evaluation data: 6327.505716080997\n",
            "Accuracy on evaluation data: 9494 / 10000\n",
            "\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 6413.562978210789\n",
            "Accuracy on training data: 47964 / 50000\n",
            "Cost on evaluation data: 6413.602897621226\n",
            "Accuracy on evaluation data: 9510 / 10000\n",
            "\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 6458.793712201737\n",
            "Accuracy on training data: 48116 / 50000\n",
            "Cost on evaluation data: 6458.825008820911\n",
            "Accuracy on evaluation data: 9592 / 10000\n",
            "\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 6510.71913062964\n",
            "Accuracy on training data: 47183 / 50000\n",
            "Cost on evaluation data: 6510.755642450649\n",
            "Accuracy on evaluation data: 9384 / 10000\n",
            "\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 6530.822006748362\n",
            "Accuracy on training data: 48153 / 50000\n",
            "Cost on evaluation data: 6530.873897469152\n",
            "Accuracy on evaluation data: 9559 / 10000\n",
            "\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 6551.412970742941\n",
            "Accuracy on training data: 48043 / 50000\n",
            "Cost on evaluation data: 6551.461431325427\n",
            "Accuracy on evaluation data: 9529 / 10000\n",
            "\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 6571.11986782403\n",
            "Accuracy on training data: 48234 / 50000\n",
            "Cost on evaluation data: 6571.167745080889\n",
            "Accuracy on evaluation data: 9585 / 10000\n",
            "\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 6548.880358019369\n",
            "Accuracy on training data: 48355 / 50000\n",
            "Cost on evaluation data: 6548.930367483888\n",
            "Accuracy on evaluation data: 9579 / 10000\n",
            "\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 6609.9966057984\n",
            "Accuracy on training data: 48187 / 50000\n",
            "Cost on evaluation data: 6610.038408960594\n",
            "Accuracy on evaluation data: 9583 / 10000\n",
            "\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 6651.849828018722\n",
            "Accuracy on training data: 48229 / 50000\n",
            "Cost on evaluation data: 6651.901626179593\n",
            "Accuracy on evaluation data: 9564 / 10000\n",
            "\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 6634.488256563289\n",
            "Accuracy on training data: 48136 / 50000\n",
            "Cost on evaluation data: 6634.534051575165\n",
            "Accuracy on evaluation data: 9544 / 10000\n",
            "\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 6656.979312152978\n",
            "Accuracy on training data: 48364 / 50000\n",
            "Cost on evaluation data: 6657.031141831679\n",
            "Accuracy on evaluation data: 9580 / 10000\n",
            "\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 6661.537916394122\n",
            "Accuracy on training data: 48042 / 50000\n",
            "Cost on evaluation data: 6661.576904005512\n",
            "Accuracy on evaluation data: 9539 / 10000\n",
            "\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 6648.248288468232\n",
            "Accuracy on training data: 48358 / 50000\n",
            "Cost on evaluation data: 6648.295175338442\n",
            "Accuracy on evaluation data: 9601 / 10000\n",
            "\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 6676.825861124745\n",
            "Accuracy on training data: 48399 / 50000\n",
            "Cost on evaluation data: 6676.874154929533\n",
            "Accuracy on evaluation data: 9597 / 10000\n",
            "\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 6691.199597173234\n",
            "Accuracy on training data: 48035 / 50000\n",
            "Cost on evaluation data: 6691.254678918254\n",
            "Accuracy on evaluation data: 9526 / 10000\n",
            "\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 6672.285648365719\n",
            "Accuracy on training data: 48401 / 50000\n",
            "Cost on evaluation data: 6672.336310079882\n",
            "Accuracy on evaluation data: 9583 / 10000\n",
            "\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 6654.675976574491\n",
            "Accuracy on training data: 48410 / 50000\n",
            "Cost on evaluation data: 6654.721511036216\n",
            "Accuracy on evaluation data: 9616 / 10000\n",
            "\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 6696.135601289496\n",
            "Accuracy on training data: 48346 / 50000\n",
            "Cost on evaluation data: 6696.183964220078\n",
            "Accuracy on evaluation data: 9580 / 10000\n",
            "\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 6730.964447171569\n",
            "Accuracy on training data: 48035 / 50000\n",
            "Cost on evaluation data: 6731.020844181744\n",
            "Accuracy on evaluation data: 9531 / 10000\n",
            "\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 6734.011028908409\n",
            "Accuracy on training data: 48112 / 50000\n",
            "Cost on evaluation data: 6734.063397297585\n",
            "Accuracy on evaluation data: 9539 / 10000\n",
            "\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 6745.029754786324\n",
            "Accuracy on training data: 48399 / 50000\n",
            "Cost on evaluation data: 6745.078422054583\n",
            "Accuracy on evaluation data: 9605 / 10000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([np.float64(3831.9193873439735),\n",
              "  np.float64(4914.682078440934),\n",
              "  np.float64(5522.40779384175),\n",
              "  np.float64(5815.904707025302),\n",
              "  np.float64(5999.153797915321),\n",
              "  np.float64(6131.958856703971),\n",
              "  np.float64(6240.129024886401),\n",
              "  np.float64(6306.160530853341),\n",
              "  np.float64(6327.505716080997),\n",
              "  np.float64(6413.602897621226),\n",
              "  np.float64(6458.825008820911),\n",
              "  np.float64(6510.755642450649),\n",
              "  np.float64(6530.873897469152),\n",
              "  np.float64(6551.461431325427),\n",
              "  np.float64(6571.167745080889),\n",
              "  np.float64(6548.930367483888),\n",
              "  np.float64(6610.038408960594),\n",
              "  np.float64(6651.901626179593),\n",
              "  np.float64(6634.534051575165),\n",
              "  np.float64(6657.031141831679),\n",
              "  np.float64(6661.576904005512),\n",
              "  np.float64(6648.295175338442),\n",
              "  np.float64(6676.874154929533),\n",
              "  np.float64(6691.254678918254),\n",
              "  np.float64(6672.336310079882),\n",
              "  np.float64(6654.721511036216),\n",
              "  np.float64(6696.183964220078),\n",
              "  np.float64(6731.020844181744),\n",
              "  np.float64(6734.063397297585),\n",
              "  np.float64(6745.078422054583)],\n",
              " [9349,\n",
              "  9477,\n",
              "  9455,\n",
              "  9538,\n",
              "  9579,\n",
              "  9566,\n",
              "  9587,\n",
              "  9562,\n",
              "  9494,\n",
              "  9510,\n",
              "  9592,\n",
              "  9384,\n",
              "  9559,\n",
              "  9529,\n",
              "  9585,\n",
              "  9579,\n",
              "  9583,\n",
              "  9564,\n",
              "  9544,\n",
              "  9580,\n",
              "  9539,\n",
              "  9601,\n",
              "  9597,\n",
              "  9526,\n",
              "  9583,\n",
              "  9616,\n",
              "  9580,\n",
              "  9531,\n",
              "  9539,\n",
              "  9605],\n",
              " [np.float64(3831.925901892078),\n",
              "  np.float64(4914.662161375608),\n",
              "  np.float64(5522.388878729542),\n",
              "  np.float64(5815.879008859292),\n",
              "  np.float64(5999.1194948603215),\n",
              "  np.float64(6131.930495662223),\n",
              "  np.float64(6240.08630934203),\n",
              "  np.float64(6306.128389551453),\n",
              "  np.float64(6327.475040966342),\n",
              "  np.float64(6413.562978210789),\n",
              "  np.float64(6458.793712201737),\n",
              "  np.float64(6510.71913062964),\n",
              "  np.float64(6530.822006748362),\n",
              "  np.float64(6551.412970742941),\n",
              "  np.float64(6571.11986782403),\n",
              "  np.float64(6548.880358019369),\n",
              "  np.float64(6609.9966057984),\n",
              "  np.float64(6651.849828018722),\n",
              "  np.float64(6634.488256563289),\n",
              "  np.float64(6656.979312152978),\n",
              "  np.float64(6661.537916394122),\n",
              "  np.float64(6648.248288468232),\n",
              "  np.float64(6676.825861124745),\n",
              "  np.float64(6691.199597173234),\n",
              "  np.float64(6672.285648365719),\n",
              "  np.float64(6654.675976574491),\n",
              "  np.float64(6696.135601289496),\n",
              "  np.float64(6730.964447171569),\n",
              "  np.float64(6734.011028908409),\n",
              "  np.float64(6745.029754786324)],\n",
              " [46640,\n",
              "  47445,\n",
              "  47501,\n",
              "  47877,\n",
              "  48086,\n",
              "  48013,\n",
              "  48118,\n",
              "  48151,\n",
              "  47726,\n",
              "  47964,\n",
              "  48116,\n",
              "  47183,\n",
              "  48153,\n",
              "  48043,\n",
              "  48234,\n",
              "  48355,\n",
              "  48187,\n",
              "  48229,\n",
              "  48136,\n",
              "  48364,\n",
              "  48042,\n",
              "  48358,\n",
              "  48399,\n",
              "  48035,\n",
              "  48401,\n",
              "  48410,\n",
              "  48346,\n",
              "  48035,\n",
              "  48112,\n",
              "  48399])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()\n",
        "netQuadratic = Network2([784, 30, 10], cost=QuadraticCost)\n",
        "netQuadratic.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)\n"
      ],
      "metadata": {
        "id": "JlPtN5Wa-fed",
        "outputId": "2d26b463-eb06-4e47-8a81-70a7cea7933c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 987.2939892399353\n",
            "Accuracy on training data: 45558 / 50000\n",
            "Cost on evaluation data: 987.2870287107669\n",
            "Accuracy on evaluation data: 9202 / 10000\n",
            "\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 1185.4514669022724\n",
            "Accuracy on training data: 46350 / 50000\n",
            "Cost on evaluation data: 1185.4472148336338\n",
            "Accuracy on evaluation data: 9329 / 10000\n",
            "\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 1285.2362468645638\n",
            "Accuracy on training data: 46826 / 50000\n",
            "Cost on evaluation data: 1285.233355958978\n",
            "Accuracy on evaluation data: 9417 / 10000\n",
            "\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 1338.3323169326623\n",
            "Accuracy on training data: 46888 / 50000\n",
            "Cost on evaluation data: 1338.329164572388\n",
            "Accuracy on evaluation data: 9415 / 10000\n",
            "\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 1377.16885429535\n",
            "Accuracy on training data: 47122 / 50000\n",
            "Cost on evaluation data: 1377.1662527454914\n",
            "Accuracy on evaluation data: 9445 / 10000\n",
            "\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 1399.6449871734824\n",
            "Accuracy on training data: 47184 / 50000\n",
            "Cost on evaluation data: 1399.6423011029533\n",
            "Accuracy on evaluation data: 9479 / 10000\n",
            "\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 1421.1914929752766\n",
            "Accuracy on training data: 47215 / 50000\n",
            "Cost on evaluation data: 1421.1890634464096\n",
            "Accuracy on evaluation data: 9487 / 10000\n",
            "\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 1438.932740914106\n",
            "Accuracy on training data: 47310 / 50000\n",
            "Cost on evaluation data: 1438.9306291236182\n",
            "Accuracy on evaluation data: 9485 / 10000\n",
            "\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 1451.8525019690267\n",
            "Accuracy on training data: 47454 / 50000\n",
            "Cost on evaluation data: 1451.8504155760427\n",
            "Accuracy on evaluation data: 9524 / 10000\n",
            "\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 1458.5370782705431\n",
            "Accuracy on training data: 47522 / 50000\n",
            "Cost on evaluation data: 1458.5350313694216\n",
            "Accuracy on evaluation data: 9539 / 10000\n",
            "\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 1465.6137273537192\n",
            "Accuracy on training data: 47285 / 50000\n",
            "Cost on evaluation data: 1465.6118444761446\n",
            "Accuracy on evaluation data: 9493 / 10000\n",
            "\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 1476.1428363218447\n",
            "Accuracy on training data: 47533 / 50000\n",
            "Cost on evaluation data: 1476.1410292394569\n",
            "Accuracy on evaluation data: 9546 / 10000\n",
            "\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 1482.8570694393372\n",
            "Accuracy on training data: 47527 / 50000\n",
            "Cost on evaluation data: 1482.8556680299052\n",
            "Accuracy on evaluation data: 9531 / 10000\n",
            "\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 1488.8812126172077\n",
            "Accuracy on training data: 47565 / 50000\n",
            "Cost on evaluation data: 1488.879090503755\n",
            "Accuracy on evaluation data: 9545 / 10000\n",
            "\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 1492.7425313644562\n",
            "Accuracy on training data: 47637 / 50000\n",
            "Cost on evaluation data: 1492.740627479023\n",
            "Accuracy on evaluation data: 9564 / 10000\n",
            "\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 1495.5871833407798\n",
            "Accuracy on training data: 47689 / 50000\n",
            "Cost on evaluation data: 1495.5855330404643\n",
            "Accuracy on evaluation data: 9560 / 10000\n",
            "\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 1498.7174668197576\n",
            "Accuracy on training data: 47638 / 50000\n",
            "Cost on evaluation data: 1498.7153548282092\n",
            "Accuracy on evaluation data: 9563 / 10000\n",
            "\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 1501.0928209735546\n",
            "Accuracy on training data: 47659 / 50000\n",
            "Cost on evaluation data: 1501.0910276399609\n",
            "Accuracy on evaluation data: 9555 / 10000\n",
            "\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 1502.0453095164933\n",
            "Accuracy on training data: 47696 / 50000\n",
            "Cost on evaluation data: 1502.0436987117787\n",
            "Accuracy on evaluation data: 9562 / 10000\n",
            "\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 1505.9524613983288\n",
            "Accuracy on training data: 47650 / 50000\n",
            "Cost on evaluation data: 1505.9506588063516\n",
            "Accuracy on evaluation data: 9551 / 10000\n",
            "\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 1508.661130149648\n",
            "Accuracy on training data: 47581 / 50000\n",
            "Cost on evaluation data: 1508.659586522587\n",
            "Accuracy on evaluation data: 9542 / 10000\n",
            "\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 1510.8683856640994\n",
            "Accuracy on training data: 47756 / 50000\n",
            "Cost on evaluation data: 1510.866824701721\n",
            "Accuracy on evaluation data: 9588 / 10000\n",
            "\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 1512.0597461415573\n",
            "Accuracy on training data: 47736 / 50000\n",
            "Cost on evaluation data: 1512.0580893313572\n",
            "Accuracy on evaluation data: 9581 / 10000\n",
            "\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 1513.1300900614963\n",
            "Accuracy on training data: 47753 / 50000\n",
            "Cost on evaluation data: 1513.128560970693\n",
            "Accuracy on evaluation data: 9566 / 10000\n",
            "\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 1512.6979637005284\n",
            "Accuracy on training data: 47801 / 50000\n",
            "Cost on evaluation data: 1512.6960708728939\n",
            "Accuracy on evaluation data: 9590 / 10000\n",
            "\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 1513.8126943117752\n",
            "Accuracy on training data: 47766 / 50000\n",
            "Cost on evaluation data: 1513.8106961505391\n",
            "Accuracy on evaluation data: 9569 / 10000\n",
            "\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 1516.3704337796744\n",
            "Accuracy on training data: 47724 / 50000\n",
            "Cost on evaluation data: 1516.3687858276799\n",
            "Accuracy on evaluation data: 9579 / 10000\n",
            "\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 1515.394965801714\n",
            "Accuracy on training data: 47788 / 50000\n",
            "Cost on evaluation data: 1515.3930591206458\n",
            "Accuracy on evaluation data: 9582 / 10000\n",
            "\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 1520.6293925560924\n",
            "Accuracy on training data: 47705 / 50000\n",
            "Cost on evaluation data: 1520.6273483173543\n",
            "Accuracy on evaluation data: 9567 / 10000\n",
            "\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 1520.3649787351605\n",
            "Accuracy on training data: 47844 / 50000\n",
            "Cost on evaluation data: 1520.363080344273\n",
            "Accuracy on evaluation data: 9600 / 10000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([np.float64(987.2870287107669),\n",
              "  np.float64(1185.4472148336338),\n",
              "  np.float64(1285.233355958978),\n",
              "  np.float64(1338.329164572388),\n",
              "  np.float64(1377.1662527454914),\n",
              "  np.float64(1399.6423011029533),\n",
              "  np.float64(1421.1890634464096),\n",
              "  np.float64(1438.9306291236182),\n",
              "  np.float64(1451.8504155760427),\n",
              "  np.float64(1458.5350313694216),\n",
              "  np.float64(1465.6118444761446),\n",
              "  np.float64(1476.1410292394569),\n",
              "  np.float64(1482.8556680299052),\n",
              "  np.float64(1488.879090503755),\n",
              "  np.float64(1492.740627479023),\n",
              "  np.float64(1495.5855330404643),\n",
              "  np.float64(1498.7153548282092),\n",
              "  np.float64(1501.0910276399609),\n",
              "  np.float64(1502.0436987117787),\n",
              "  np.float64(1505.9506588063516),\n",
              "  np.float64(1508.659586522587),\n",
              "  np.float64(1510.866824701721),\n",
              "  np.float64(1512.0580893313572),\n",
              "  np.float64(1513.128560970693),\n",
              "  np.float64(1512.6960708728939),\n",
              "  np.float64(1513.8106961505391),\n",
              "  np.float64(1516.3687858276799),\n",
              "  np.float64(1515.3930591206458),\n",
              "  np.float64(1520.6273483173543),\n",
              "  np.float64(1520.363080344273)],\n",
              " [9202,\n",
              "  9329,\n",
              "  9417,\n",
              "  9415,\n",
              "  9445,\n",
              "  9479,\n",
              "  9487,\n",
              "  9485,\n",
              "  9524,\n",
              "  9539,\n",
              "  9493,\n",
              "  9546,\n",
              "  9531,\n",
              "  9545,\n",
              "  9564,\n",
              "  9560,\n",
              "  9563,\n",
              "  9555,\n",
              "  9562,\n",
              "  9551,\n",
              "  9542,\n",
              "  9588,\n",
              "  9581,\n",
              "  9566,\n",
              "  9590,\n",
              "  9569,\n",
              "  9579,\n",
              "  9582,\n",
              "  9567,\n",
              "  9600],\n",
              " [np.float64(987.2939892399353),\n",
              "  np.float64(1185.4514669022724),\n",
              "  np.float64(1285.2362468645638),\n",
              "  np.float64(1338.3323169326623),\n",
              "  np.float64(1377.16885429535),\n",
              "  np.float64(1399.6449871734824),\n",
              "  np.float64(1421.1914929752766),\n",
              "  np.float64(1438.932740914106),\n",
              "  np.float64(1451.8525019690267),\n",
              "  np.float64(1458.5370782705431),\n",
              "  np.float64(1465.6137273537192),\n",
              "  np.float64(1476.1428363218447),\n",
              "  np.float64(1482.8570694393372),\n",
              "  np.float64(1488.8812126172077),\n",
              "  np.float64(1492.7425313644562),\n",
              "  np.float64(1495.5871833407798),\n",
              "  np.float64(1498.7174668197576),\n",
              "  np.float64(1501.0928209735546),\n",
              "  np.float64(1502.0453095164933),\n",
              "  np.float64(1505.9524613983288),\n",
              "  np.float64(1508.661130149648),\n",
              "  np.float64(1510.8683856640994),\n",
              "  np.float64(1512.0597461415573),\n",
              "  np.float64(1513.1300900614963),\n",
              "  np.float64(1512.6979637005284),\n",
              "  np.float64(1513.8126943117752),\n",
              "  np.float64(1516.3704337796744),\n",
              "  np.float64(1515.394965801714),\n",
              "  np.float64(1520.6293925560924),\n",
              "  np.float64(1520.3649787351605)],\n",
              " [45558,\n",
              "  46350,\n",
              "  46826,\n",
              "  46888,\n",
              "  47122,\n",
              "  47184,\n",
              "  47215,\n",
              "  47310,\n",
              "  47454,\n",
              "  47522,\n",
              "  47285,\n",
              "  47533,\n",
              "  47527,\n",
              "  47565,\n",
              "  47637,\n",
              "  47689,\n",
              "  47638,\n",
              "  47659,\n",
              "  47696,\n",
              "  47650,\n",
              "  47581,\n",
              "  47756,\n",
              "  47736,\n",
              "  47753,\n",
              "  47801,\n",
              "  47766,\n",
              "  47724,\n",
              "  47788,\n",
              "  47705,\n",
              "  47844])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "MNielsen_network.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}