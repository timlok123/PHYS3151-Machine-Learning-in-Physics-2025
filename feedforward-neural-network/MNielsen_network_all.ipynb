{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timlok123/PHYS3151-Machine-Learning-in-Physics-2025/blob/main/feedforward-neural-network/MNielsen_network_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dXoM7ZwcSdGv"
      },
      "outputs": [],
      "source": [
        "# A module to implement the gradient descent learning algorithm for a feedforward neural network.\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "\n",
        "    # the list ''sizes'' contains the number of neurons in the respective layers of the network.\n",
        "    # [2, 3, 1] input layer 2 neurons, hidden layer 3 neurons, output layer 1 neuron\n",
        "    def __init__(self, sizes):\n",
        "\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    # return the output of the network if \"a\" is input.\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    # “training_data” is a list of tuples \"(x,y)\" representing the training inputs and the desired outputs.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
        "            else:\n",
        "                print(\"Epoch {} complete\".format(j))\n",
        "\n",
        "    # update the network's weights and biases by applying gradient descent\n",
        "    # using backpropagation to a single mini batch,i.e., for each mini_batch we apply a single step of gradient descent.\n",
        "    # Therefore, to have sufficient update of the weights, it is good to have smaller mini_batch_size.\n",
        "    # The ''mini_batch'' is a list of tuples \"(x,y)\", and \"eta\" is the learning rate.\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                       for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # l=1 means the last layer of the neurons\n",
        "        # l=2 means the second-last layer\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    # the neural network's output is assumed to be the index of\n",
        "    # whichever neuron in the final layer has the highest activation.\n",
        "    def evaluate(self, test_data):\n",
        "        test_results = [(np.argmax(self.feedforward(x)),y)\n",
        "                       for (x,y) in test_data]\n",
        "        return sum(int(x == y) for (x,y) in test_results)\n",
        "\n",
        "    # vector of partial derivatives \\partial C_X / \\partial a for the output activations.\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "\n",
        "# Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Therefore, to have sufficient update of the weights, it is good to have smaller mini_batch_size**."
      ],
      "metadata": {
        "id": "oWvHo56pn1oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neural_network = Network([2,3,2,1])\n",
        "# show the random biases\n",
        "neural_network.biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg7xJiLiBUSl",
        "outputId": "22f950de-64da-401a-bdca-474938d4e123"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-1.832568  ],\n",
              "        [ 0.06666685],\n",
              "        [-0.88062396]]),\n",
              " array([[-0.38639142],\n",
              "        [ 0.04913843]]),\n",
              " array([[0.22862699]])]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show the random weights\n",
        "neural_network.weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYzcbJs2CMQ-",
        "outputId": "bea1ec1c-88e5-4bdf-a5d4-57ef9a69ba15"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.41840638,  0.11038804],\n",
              "        [-1.52677778,  0.96398362],\n",
              "        [ 0.60366675, -0.3656712 ]]),\n",
              " array([[ 0.87989904, -0.62332928,  0.38436381],\n",
              "        [ 0.9400919 , -1.06177889, -0.24222848]]),\n",
              " array([[-0.08389149, -0.21856436]])]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/timlok123/PHYS3151-Machine-Learning-in-Physics-2025.git"
      ],
      "metadata": {
        "id": "RoEQB1tkTO4i",
        "outputId": "4e341943-7bf0-4fa6-d1cf-d2765bd33280",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'PHYS3151-Machine-Learning-in-Physics-2025' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9bt3SXZUSdGy"
      },
      "outputs": [],
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is,\n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2025/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\".\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector\n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "\n",
        "    plt.imshow(training_inputs[4999].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a 10-dimensional unit vector with a 1.0 in the jth position and zeros elsewhere.\n",
        "# This is used to convert a digit (0...9) into a corresponding desired output from the neural network.\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5k-jTJDrSdGy",
        "outputId": "dc88a9ad-610c-4131-8d6d-4d2b1a7cb228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/PHYS3151-Machine-Learning-in-Physics-2024/feedforward-neural-network/mnist.pkl.gz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5733424cb514>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-e85be037ef81>\u001b[0m in \u001b[0;36mload_data_wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtr_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mtraining_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtr_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-e85be037ef81>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/PHYS3151-Machine-Learning-in-Physics-2024/feedforward-neural-network/mnist.pkl.gz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/PHYS3151-Machine-Learning-in-Physics-2024/feedforward-neural-network/mnist.pkl.gz'"
          ]
        }
      ],
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqQPTo_mSdGz"
      },
      "outputs": [],
      "source": [
        "net = Network([784,30,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2frayMZSdGz"
      },
      "outputs": [],
      "source": [
        "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRs0ll3ISdGz"
      },
      "outputs": [],
      "source": [
        "# An improved version of MNielsen_network, implementing the SGD learning algorithm for a feedforward neural network.\n",
        "# Improvments include\n",
        "# 1. the cross-entropy cost function,\n",
        "# 2. regularization,\n",
        "# 3. better initialization of the weights.\n",
        "\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# define the quadratic and cross-entropy cost functions\n",
        "\n",
        "class QuadraticCost(object):\n",
        "\n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return 0.5*np.linalg.norm(a-y)**2\n",
        "\n",
        "    # return the error delta from the output layer.\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)*sigmoid_prime(z)\n",
        "\n",
        "class CrossEntropyCost(object):\n",
        "\n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    # np.nan_to_num is used to ensure numerical stability, make sure 0*log(0) = 0.0\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "\n",
        "    # returm the error delta from the output layer.\n",
        "    # parameter \"z\" is not used by the method.\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)\n",
        "\n",
        "# Main Network class\n",
        "class Network2(object):\n",
        "\n",
        "\n",
        "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "\n",
        "        # the biases and weights for the network are initiated randomly, using \"self.default_weight_initializer\".\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.default_weight_initializer()\n",
        "        self.cost = cost\n",
        "\n",
        "    def default_weight_initializer(self):\n",
        "\n",
        "        # initialize each weight using a Gaussian distribution with mean 0 and standard deviation 1\n",
        "        # over the square root of the number of weights connecting to the same neuron.\n",
        "        # initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "\n",
        "        # for the input layers, there is no biases.\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def large_weight_initializer(self):\n",
        "\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    # \"evaluation_data\", monitor the cost and accuracy on either the evaluation data or the training data.\n",
        "    # returns a tuple containing four lists:\n",
        "    # 1. the (per-epoch) costs on the evaluation data,\n",
        "    # 2. the accuracies on the evaluation data,\n",
        "    # 3. the costs on the training data,\n",
        "    # 4. the accuracies on the training data.\n",
        "    # If we train for 30 epochs, then the first element of the tuple will be a\n",
        "    # 30-element list containing the cost on the evaluation data at the end of each epoch.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            lmbda=0.0,\n",
        "            evaluation_data = None,\n",
        "            monitor_evaluation_cost = False,\n",
        "            monitor_evaluation_accuracy = False,\n",
        "            monitor_training_cost = False,\n",
        "            monitor_training_accuracy = False,\n",
        "            early_stopping_n = 0):\n",
        "\n",
        "        # early stopping functionality:\n",
        "        best_accuracy=1\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if evaluation_data:\n",
        "            evaluation_data = list(evaluation_data)\n",
        "            n_data = len(evaluation_data)\n",
        "\n",
        "        # early stopping functionality:\n",
        "        best_accuracy=0\n",
        "        no_accuracy_change=0\n",
        "\n",
        "        evaluation_cost, evaluation_accuracy = [], []\n",
        "        training_cost, training_accuracy = [], []\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "\n",
        "            print(\"Epoch %s training complete\" % j)\n",
        "\n",
        "            if monitor_training_cost:\n",
        "                cost = self.total_cost(training_data, lmbda)\n",
        "                training_cost.append(cost)\n",
        "                print(\"Cost on training data: {}\".format(cost))\n",
        "            if monitor_training_accuracy:\n",
        "                accuracy = self.accuracy(training_data, convert=True)\n",
        "                training_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on training data: {} / {}\".format(accuracy, n))\n",
        "            if monitor_evaluation_cost:\n",
        "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                evaluation_cost.append(cost)\n",
        "                print(\"Cost on evaluation data: {}\".format(cost))\n",
        "            if monitor_evaluation_accuracy:\n",
        "                accuracy = self.accuracy(evaluation_data)\n",
        "                evaluation_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
        "\n",
        "            print()\n",
        "\n",
        "            # Early stopping:\n",
        "            if early_stopping_n > 0:\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    no_accuracy_change = 0\n",
        "                    print(\"Early-stopping: Best so far{}\".format(best_accuracy))\n",
        "                else:\n",
        "                    no_accuracy_change += 1\n",
        "\n",
        "                if (no_accuracy_change == early_stopping_n):\n",
        "                    print(\"Early-stopping: No accuracy cahnge in last epochs: {}\".format(early_stopping_n))\n",
        "                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "\n",
        "    # \"mini_batch\" is a list of tuples \"(x,y)\"\n",
        "    # \"eta\" is the learning rate\n",
        "    # \"lmbda\" is the regularization parameter\n",
        "    # \"n\" is the total size of the training set.\n",
        "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        # \"eta*(lmbda/n)*w\" comes from the regularization term.\n",
        "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # backpropagate\n",
        "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "\n",
        "    def accuracy(self, data, convert=False):\n",
        "\n",
        "        if convert:\n",
        "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
        "        else:\n",
        "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
        "\n",
        "        result_accuracy = sum(int(x==y) for (x,y) in results)\n",
        "        return result_accuracy\n",
        "\n",
        "    def total_cost(self, data, lmbda, convert=False):\n",
        "        cost = 0.0\n",
        "        for x, y in data:\n",
        "            a = self.feedforward(x)\n",
        "            if convert: y = vectorized_result(y)\n",
        "            cost += self.cost.fn(a, y)/len(data)\n",
        "            cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
        "        return cost\n",
        "\n",
        "    # Save the neural network to the file \"filename\".\n",
        "    def save(self, filename):\n",
        "        data = {\"sizes\": self.sizes,\n",
        "                \"weights\": [w.tolist() for w in self.weights],\n",
        "                \"biases\": [b.tolist() for b in self.biases],\n",
        "                \"cost\": str(self.cost.__name__)}\n",
        "        f = open(filename, \"w\")\n",
        "        json.dump(data, f)\n",
        "        f.close()\n",
        "\n",
        "\n",
        "# Loading a Network\n",
        "def load(filename):\n",
        "\n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "\n",
        "# Miscellaneous functions\n",
        "def vectorized_result(j):\n",
        "\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is,\n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2024/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\".\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector\n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "\n",
        "    plt.imshow(training_inputs[1].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)"
      ],
      "metadata": {
        "id": "4KCSryRc-XA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ],
      "metadata": {
        "id": "czAce4vR-Y_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = Network2([784, 30, 10], cost=CrossEntropyCost)\n",
        "net2.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)"
      ],
      "metadata": {
        "id": "m3vhgcz_-bEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()\n",
        "netQuadratic = Network2([784, 30, 10], cost=QuadraticCost)\n",
        "netQuadratic.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)\n"
      ],
      "metadata": {
        "id": "JlPtN5Wa-fed"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "MNielsen_network.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}