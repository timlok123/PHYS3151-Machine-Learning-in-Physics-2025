{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timlok123/PHYS3151-Machine-Learning-in-Physics-2025/blob/main/feedforward-neural-network/MNielsen_network_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dXoM7ZwcSdGv"
      },
      "outputs": [],
      "source": [
        "# A module to implement the gradient descent learning algorithm for a feedforward neural network.\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "\n",
        "    # the list ''sizes'' contains the number of neurons in the respective layers of the network.\n",
        "    # [2, 3, 1] input layer 2 neurons, hidden layer 3 neurons, output layer 1 neuron\n",
        "    def __init__(self, sizes):\n",
        "\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    # return the output of the network if \"a\" is input.\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    # “training_data” is a list of tuples \"(x,y)\" representing the training inputs and the desired outputs.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
        "            else:\n",
        "                print(\"Epoch {} complete\".format(j))\n",
        "\n",
        "    # update the network's weights and biases by applying gradient descent\n",
        "    # using backpropagation to a single mini batch,i.e., for each mini_batch we apply a single step of gradient descent.\n",
        "    # Therefore, to have sufficient update of the weights, it is good to have smaller mini_batch_size.\n",
        "    # The ''mini_batch'' is a list of tuples \"(x,y)\", and \"eta\" is the learning rate.\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                       for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # l=1 means the last layer of the neurons\n",
        "        # l=2 means the second-last layer\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    # the neural network's output is assumed to be the index of\n",
        "    # whichever neuron in the final layer has the highest activation.\n",
        "    def evaluate(self, test_data):\n",
        "        test_results = [(np.argmax(self.feedforward(x)),y)\n",
        "                       for (x,y) in test_data]\n",
        "        return sum(int(x == y) for (x,y) in test_results)\n",
        "\n",
        "    # vector of partial derivatives \\partial C_X / \\partial a for the output activations.\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "\n",
        "# Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Therefore, to have sufficient update of the weights, it is good to have smaller mini_batch_size**."
      ],
      "metadata": {
        "id": "oWvHo56pn1oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neural_network = Network([2,3,2,1])\n",
        "# show the random biases\n",
        "neural_network.biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg7xJiLiBUSl",
        "outputId": "cd9616d8-2202-4dff-ecf4-4be0f55535c8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.35999653],\n",
              "        [ 2.16351283],\n",
              "        [-0.84273227]]),\n",
              " array([[-0.07132309],\n",
              "        [ 0.23309046]]),\n",
              " array([[1.036205]])]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show the random weights\n",
        "neural_network.weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYzcbJs2CMQ-",
        "outputId": "87b70c5f-5ecd-4864-a21f-de4019daaf42"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.12173233,  0.93685389],\n",
              "        [-1.38610913, -1.19432808],\n",
              "        [ 2.25977023,  0.30844759]]),\n",
              " array([[ 0.60845476,  0.08013774, -0.02422009],\n",
              "        [-1.2052973 , -0.73102866,  0.82144504]]),\n",
              " array([[-1.66703652, -2.28966238]])]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/timlok123/PHYS3151-Machine-Learning-in-Physics-2025.git"
      ],
      "metadata": {
        "id": "RoEQB1tkTO4i",
        "outputId": "db4d767f-6452-4b63-c96f-58dd8c38660b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'PHYS3151-Machine-Learning-in-Physics-2025' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9bt3SXZUSdGy"
      },
      "outputs": [],
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is,\n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2025/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\".\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector\n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "\n",
        "    plt.imshow(training_inputs[4999].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a 10-dimensional unit vector with a 1.0 in the jth position and zeros elsewhere.\n",
        "# This is used to convert a digit (0...9) into a corresponding desired output from the neural network.\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5k-jTJDrSdGy",
        "outputId": "8a10c4bb-d0cb-4beb-937f-3d107deed1b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "iqQPTo_mSdGz"
      },
      "outputs": [],
      "source": [
        "net = Network([784,30,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "k2frayMZSdGz",
        "outputId": "dac528ec-15f5-478f-8484-cc93e7d88db4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 : 8176 / 10000\n",
            "Epoch 1 : 8288 / 10000\n",
            "Epoch 2 : 8389 / 10000\n",
            "Epoch 3 : 8413 / 10000\n",
            "Epoch 4 : 8459 / 10000\n",
            "Epoch 5 : 8461 / 10000\n",
            "Epoch 6 : 8500 / 10000\n",
            "Epoch 7 : 8471 / 10000\n",
            "Epoch 8 : 8534 / 10000\n",
            "Epoch 9 : 8538 / 10000\n",
            "Epoch 10 : 8543 / 10000\n",
            "Epoch 11 : 8593 / 10000\n",
            "Epoch 12 : 9443 / 10000\n",
            "Epoch 13 : 9497 / 10000\n",
            "Epoch 14 : 9460 / 10000\n",
            "Epoch 15 : 9489 / 10000\n",
            "Epoch 16 : 9480 / 10000\n",
            "Epoch 17 : 9475 / 10000\n",
            "Epoch 18 : 9511 / 10000\n",
            "Epoch 19 : 9497 / 10000\n",
            "Epoch 20 : 9480 / 10000\n",
            "Epoch 21 : 9499 / 10000\n",
            "Epoch 22 : 9505 / 10000\n",
            "Epoch 23 : 9487 / 10000\n",
            "Epoch 24 : 9487 / 10000\n",
            "Epoch 25 : 9500 / 10000\n",
            "Epoch 26 : 9521 / 10000\n",
            "Epoch 27 : 9488 / 10000\n",
            "Epoch 28 : 9493 / 10000\n",
            "Epoch 29 : 9494 / 10000\n"
          ]
        }
      ],
      "source": [
        "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LRs0ll3ISdGz"
      },
      "outputs": [],
      "source": [
        "# An improved version of MNielsen_network, implementing the SGD learning algorithm for a feedforward neural network.\n",
        "# Improvments include\n",
        "# 1. the cross-entropy cost function,\n",
        "# 2. regularization,\n",
        "# 3. better initialization of the weights.\n",
        "\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# define the quadratic and cross-entropy cost functions\n",
        "\n",
        "class QuadraticCost(object):\n",
        "\n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return 0.5*np.linalg.norm(a-y)**2\n",
        "\n",
        "    # return the error delta from the output layer.\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)*sigmoid_prime(z)\n",
        "\n",
        "class CrossEntropyCost(object):\n",
        "\n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    # np.nan_to_num is used to ensure numerical stability, make sure 0*log(0) = 0.0\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "\n",
        "    # returm the error delta from the output layer.\n",
        "    # parameter \"z\" is not used by the method.\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)\n",
        "\n",
        "# Main Network class\n",
        "class Network2(object):\n",
        "\n",
        "\n",
        "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "\n",
        "        # the biases and weights for the network are initiated randomly, using \"self.default_weight_initializer\".\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.default_weight_initializer()\n",
        "        self.cost = cost\n",
        "\n",
        "    def default_weight_initializer(self):\n",
        "\n",
        "        # initialize each weight using a Gaussian distribution with mean 0 and standard deviation 1\n",
        "        # over the square root of the number of weights connecting to the same neuron.\n",
        "        # initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "\n",
        "        # for the input layers, there is no biases.\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def large_weight_initializer(self):\n",
        "\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    # \"evaluation_data\", monitor the cost and accuracy on either the evaluation data or the training data.\n",
        "    # returns a tuple containing four lists:\n",
        "    # 1. the (per-epoch) costs on the evaluation data,\n",
        "    # 2. the accuracies on the evaluation data,\n",
        "    # 3. the costs on the training data,\n",
        "    # 4. the accuracies on the training data.\n",
        "    # If we train for 30 epochs, then the first element of the tuple will be a\n",
        "    # 30-element list containing the cost on the evaluation data at the end of each epoch.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            lmbda=0.0,\n",
        "            evaluation_data = None,\n",
        "            monitor_evaluation_cost = False,\n",
        "            monitor_evaluation_accuracy = False,\n",
        "            monitor_training_cost = False,\n",
        "            monitor_training_accuracy = False,\n",
        "            early_stopping_n = 0):\n",
        "\n",
        "        # early stopping functionality:\n",
        "        best_accuracy=1\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if evaluation_data:\n",
        "            evaluation_data = list(evaluation_data)\n",
        "            n_data = len(evaluation_data)\n",
        "\n",
        "        # early stopping functionality:\n",
        "        best_accuracy=0\n",
        "        no_accuracy_change=0\n",
        "\n",
        "        evaluation_cost, evaluation_accuracy = [], []\n",
        "        training_cost, training_accuracy = [], []\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "\n",
        "            print(\"Epoch %s training complete\" % j)\n",
        "\n",
        "            if monitor_training_cost:\n",
        "                cost = self.total_cost(training_data, lmbda)\n",
        "                training_cost.append(cost)\n",
        "                print(\"Cost on training data: {}\".format(cost))\n",
        "            if monitor_training_accuracy:\n",
        "                accuracy = self.accuracy(training_data, convert=True)\n",
        "                training_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on training data: {} / {}\".format(accuracy, n))\n",
        "            if monitor_evaluation_cost:\n",
        "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                evaluation_cost.append(cost)\n",
        "                print(\"Cost on evaluation data: {}\".format(cost))\n",
        "            if monitor_evaluation_accuracy:\n",
        "                accuracy = self.accuracy(evaluation_data)\n",
        "                evaluation_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
        "\n",
        "            print()\n",
        "\n",
        "            # Early stopping:\n",
        "            if early_stopping_n > 0:\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    no_accuracy_change = 0\n",
        "                    print(\"Early-stopping: Best so far{}\".format(best_accuracy))\n",
        "                else:\n",
        "                    no_accuracy_change += 1\n",
        "\n",
        "                if (no_accuracy_change == early_stopping_n):\n",
        "                    print(\"Early-stopping: No accuracy cahnge in last epochs: {}\".format(early_stopping_n))\n",
        "                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "\n",
        "    # \"mini_batch\" is a list of tuples \"(x,y)\"\n",
        "    # \"eta\" is the learning rate\n",
        "    # \"lmbda\" is the regularization parameter\n",
        "    # \"n\" is the total size of the training set.\n",
        "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        # \"eta*(lmbda/n)*w\" comes from the regularization term.\n",
        "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # backpropagate\n",
        "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "\n",
        "    def accuracy(self, data, convert=False):\n",
        "\n",
        "        if convert:\n",
        "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
        "        else:\n",
        "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
        "\n",
        "        result_accuracy = sum(int(x==y) for (x,y) in results)\n",
        "        return result_accuracy\n",
        "\n",
        "    def total_cost(self, data, lmbda, convert=False):\n",
        "        cost = 0.0\n",
        "        for x, y in data:\n",
        "            a = self.feedforward(x)\n",
        "            if convert: y = vectorized_result(y)\n",
        "            cost += self.cost.fn(a, y)/len(data)\n",
        "            cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
        "        return cost\n",
        "\n",
        "    # Save the neural network to the file \"filename\".\n",
        "    def save(self, filename):\n",
        "        data = {\"sizes\": self.sizes,\n",
        "                \"weights\": [w.tolist() for w in self.weights],\n",
        "                \"biases\": [b.tolist() for b in self.biases],\n",
        "                \"cost\": str(self.cost.__name__)}\n",
        "        f = open(filename, \"w\")\n",
        "        json.dump(data, f)\n",
        "        f.close()\n",
        "\n",
        "\n",
        "# Loading a Network\n",
        "def load(filename):\n",
        "\n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "\n",
        "# Miscellaneous functions\n",
        "def vectorized_result(j):\n",
        "\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is,\n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2025/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\".\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector\n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "\n",
        "    plt.imshow(training_inputs[1].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)"
      ],
      "metadata": {
        "id": "4KCSryRc-XA5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ],
      "metadata": {
        "id": "czAce4vR-Y_-",
        "outputId": "587ec9dc-9169-4a42-82a4-5e64389fbbe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/PHYS3151-Machine-Learning-in-Physics-2024/feedforward-neural-network/mnist.pkl.gz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-5733424cb514>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-e7763fe7350c>\u001b[0m in \u001b[0;36mload_data_wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtr_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mtraining_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtr_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-e7763fe7350c>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/PHYS3151-Machine-Learning-in-Physics-2024/feedforward-neural-network/mnist.pkl.gz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/PHYS3151-Machine-Learning-in-Physics-2024/feedforward-neural-network/mnist.pkl.gz'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = Network2([784, 30, 10], cost=CrossEntropyCost)\n",
        "net2.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)"
      ],
      "metadata": {
        "id": "m3vhgcz_-bEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()\n",
        "netQuadratic = Network2([784, 30, 10], cost=QuadraticCost)\n",
        "netQuadratic.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)\n"
      ],
      "metadata": {
        "id": "JlPtN5Wa-fed"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "MNielsen_network.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}