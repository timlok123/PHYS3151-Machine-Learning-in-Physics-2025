{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidGoing/PHYS3151-Machine-Learning-in-Physics-2024/blob/main/feedforward-neural-network/MNielsen_network_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dXoM7ZwcSdGv"
      },
      "outputs": [],
      "source": [
        "# A module to implement the gradient descent learning algorithm for a feedforward neural network.\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "\n",
        "    # the list ''sizes'' contains the number of neurons in the respective layers of the network.\n",
        "    # [2, 3, 1] input layer 2 neurons, hidden layer 3 neurons, output layer 1 neuron\n",
        "    def __init__(self, sizes):\n",
        "\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    # return the output of the network if \"a\" is input.\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    # “training_data” is a list of tuples \"(x,y)\" representing the training inputs and the desired outputs.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
        "            else:\n",
        "                print(\"Epoch {} complete\".format(j))\n",
        "\n",
        "    # update the network's weights and biases by applying gradient descent\n",
        "    # using backpropagation to a single mini batch,i.e., for each mini_batch we apply a single step of gradient descent.\n",
        "    # Therefore, to have sufficient update of the weights, it is good to have smaller mini_batch_size.\n",
        "    # The ''mini_batch'' is a list of tuples \"(x,y)\", and \"eta\" is the learning rate.\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                       for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # l=1 means the last layer of the neurons\n",
        "        # l=2 means the second-last layer\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    # the neural network's output is assumed to be the index of\n",
        "    # whichever neuron in the final layer has the highest activation.\n",
        "    def evaluate(self, test_data):\n",
        "        test_results = [(np.argmax(self.feedforward(x)),y)\n",
        "                       for (x,y) in test_data]\n",
        "        return sum(int(x == y) for (x,y) in test_results)\n",
        "\n",
        "    # vector of partial derivatives \\partial C_X / \\partial a for the output activations.\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "\n",
        "# Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Therefore, to have sufficient update of the weights, it is good to have smaller mini_batch_size**."
      ],
      "metadata": {
        "id": "oWvHo56pn1oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neural_network = Network([2,3,2,1])\n",
        "# show the random biases\n",
        "neural_network.biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg7xJiLiBUSl",
        "outputId": "f96f4005-38eb-46f7-e3aa-6d8a6cf8ca0d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.31219582],\n",
              "        [ 0.23022609],\n",
              "        [-0.42637157]]),\n",
              " array([[-0.69164565],\n",
              "        [-0.71215426]]),\n",
              " array([[-0.19161595]])]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show the random weights\n",
        "neural_network.weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYzcbJs2CMQ-",
        "outputId": "4b2307bb-2dba-41e4-a190-3c941e3fdaa8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-1.32336356, -0.04512131],\n",
              "        [ 0.03194098,  0.43901906],\n",
              "        [-0.58214668, -0.25928345]]),\n",
              " array([[-0.43850529,  0.01042295,  0.37271409],\n",
              "        [ 1.84526918, -0.90997786,  0.41893705]]),\n",
              " array([[-0.65770452,  0.65280182]])]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DavidGoing/PHYS3151-Machine-Learning-in-Physics-2024"
      ],
      "metadata": {
        "id": "RoEQB1tkTO4i",
        "outputId": "3f10aa22-5234-424c-9985-e5381b26ee2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PHYS3151-Machine-Learning-in-Physics-2024'...\n",
            "remote: Enumerating objects: 773, done.\u001b[K\n",
            "remote: Counting objects: 100% (356/356), done.\u001b[K\n",
            "remote: Compressing objects: 100% (222/222), done.\u001b[K\n",
            "remote: Total 773 (delta 199), reused 232 (delta 134), pack-reused 417\u001b[K\n",
            "Receiving objects: 100% (773/773), 36.92 MiB | 19.26 MiB/s, done.\n",
            "Resolving deltas: 100% (416/416), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9bt3SXZUSdGy"
      },
      "outputs": [],
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is,\n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2024/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\".\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector\n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "\n",
        "    plt.imshow(training_inputs[4999].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a 10-dimensional unit vector with a 1.0 in the jth position and zeros elsewhere.\n",
        "# This is used to convert a digit (0...9) into a corresponding desired output from the neural network.\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5k-jTJDrSdGy",
        "outputId": "ea14260f-81ad-4d64-d6eb-16513a4db2c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iqQPTo_mSdGz"
      },
      "outputs": [],
      "source": [
        "net = Network([784,30,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k2frayMZSdGz",
        "outputId": "bc43c2b2-5957-4819-b92d-62ea76829b86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 : 9062 / 10000\n",
            "Epoch 1 : 9233 / 10000\n",
            "Epoch 2 : 9313 / 10000\n",
            "Epoch 3 : 9334 / 10000\n",
            "Epoch 4 : 9380 / 10000\n",
            "Epoch 5 : 9437 / 10000\n",
            "Epoch 6 : 9434 / 10000\n",
            "Epoch 7 : 9440 / 10000\n",
            "Epoch 8 : 9419 / 10000\n",
            "Epoch 9 : 9472 / 10000\n",
            "Epoch 10 : 9435 / 10000\n",
            "Epoch 11 : 9462 / 10000\n",
            "Epoch 12 : 9507 / 10000\n",
            "Epoch 13 : 9505 / 10000\n",
            "Epoch 14 : 9507 / 10000\n",
            "Epoch 15 : 9451 / 10000\n",
            "Epoch 16 : 9475 / 10000\n",
            "Epoch 17 : 9480 / 10000\n",
            "Epoch 18 : 9496 / 10000\n",
            "Epoch 19 : 9460 / 10000\n",
            "Epoch 20 : 9520 / 10000\n",
            "Epoch 21 : 9499 / 10000\n",
            "Epoch 22 : 9513 / 10000\n",
            "Epoch 23 : 9506 / 10000\n",
            "Epoch 24 : 9534 / 10000\n",
            "Epoch 25 : 9510 / 10000\n",
            "Epoch 26 : 9515 / 10000\n",
            "Epoch 27 : 9511 / 10000\n",
            "Epoch 28 : 9508 / 10000\n",
            "Epoch 29 : 9523 / 10000\n"
          ]
        }
      ],
      "source": [
        "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LRs0ll3ISdGz"
      },
      "outputs": [],
      "source": [
        "# An improved version of MNielsen_network, implementing the SGD learning algorithm for a feedforward neural network.\n",
        "# Improvments include\n",
        "# 1. the cross-entropy cost function,\n",
        "# 2. regularization,\n",
        "# 3. better initialization of the weights.\n",
        "\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# define the quadratic and cross-entropy cost functions\n",
        "\n",
        "class QuadraticCost(object):\n",
        "\n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return 0.5*np.linalg.norm(a-y)**2\n",
        "\n",
        "    # return the error delta from the output layer.\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)*sigmoid_prime(z)\n",
        "\n",
        "class CrossEntropyCost(object):\n",
        "\n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    # np.nan_to_num is used to ensure numerical stability, make sure 0*log(0) = 0.0\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "\n",
        "    # returm the error delta from the output layer.\n",
        "    # parameter \"z\" is not used by the method.\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)\n",
        "\n",
        "# Main Network class\n",
        "class Network2(object):\n",
        "\n",
        "\n",
        "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "\n",
        "        # the biases and weights for the network are initiated randomly, using \"self.default_weight_initializer\".\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.default_weight_initializer()\n",
        "        self.cost = cost\n",
        "\n",
        "    def default_weight_initializer(self):\n",
        "\n",
        "        # initialize each weight using a Gaussian distribution with mean 0 and standard deviation 1\n",
        "        # over the square root of the number of weights connecting to the same neuron.\n",
        "        # initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "\n",
        "        # for the input layers, there is no biases.\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def large_weight_initializer(self):\n",
        "\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    # \"evaluation_data\", monitor the cost and accuracy on either the evaluation data or the training data.\n",
        "    # returns a tuple containing four lists:\n",
        "    # 1. the (per-epoch) costs on the evaluation data,\n",
        "    # 2. the accuracies on the evaluation data,\n",
        "    # 3. the costs on the training data,\n",
        "    # 4. the accuracies on the training data.\n",
        "    # If we train for 30 epochs, then the first element of the tuple will be a\n",
        "    # 30-element list containing the cost on the evaluation data at the end of each epoch.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            lmbda=0.0,\n",
        "            evaluation_data = None,\n",
        "            monitor_evaluation_cost = False,\n",
        "            monitor_evaluation_accuracy = False,\n",
        "            monitor_training_cost = False,\n",
        "            monitor_training_accuracy = False,\n",
        "            early_stopping_n = 0):\n",
        "\n",
        "        # early stopping functionality:\n",
        "        best_accuracy=1\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        if evaluation_data:\n",
        "            evaluation_data = list(evaluation_data)\n",
        "            n_data = len(evaluation_data)\n",
        "\n",
        "        # early stopping functionality:\n",
        "        best_accuracy=0\n",
        "        no_accuracy_change=0\n",
        "\n",
        "        evaluation_cost, evaluation_accuracy = [], []\n",
        "        training_cost, training_accuracy = [], []\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "\n",
        "            print(\"Epoch %s training complete\" % j)\n",
        "\n",
        "            if monitor_training_cost:\n",
        "                cost = self.total_cost(training_data, lmbda)\n",
        "                training_cost.append(cost)\n",
        "                print(\"Cost on training data: {}\".format(cost))\n",
        "            if monitor_training_accuracy:\n",
        "                accuracy = self.accuracy(training_data, convert=True)\n",
        "                training_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on training data: {} / {}\".format(accuracy, n))\n",
        "            if monitor_evaluation_cost:\n",
        "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                evaluation_cost.append(cost)\n",
        "                print(\"Cost on evaluation data: {}\".format(cost))\n",
        "            if monitor_evaluation_accuracy:\n",
        "                accuracy = self.accuracy(evaluation_data)\n",
        "                evaluation_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
        "\n",
        "            print()\n",
        "\n",
        "            # Early stopping:\n",
        "            if early_stopping_n > 0:\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    no_accuracy_change = 0\n",
        "                    print(\"Early-stopping: Best so far{}\".format(best_accuracy))\n",
        "                else:\n",
        "                    no_accuracy_change += 1\n",
        "\n",
        "                if (no_accuracy_change == early_stopping_n):\n",
        "                    print(\"Early-stopping: No accuracy cahnge in last epochs: {}\".format(early_stopping_n))\n",
        "                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "\n",
        "    # \"mini_batch\" is a list of tuples \"(x,y)\"\n",
        "    # \"eta\" is the learning rate\n",
        "    # \"lmbda\" is the regularization parameter\n",
        "    # \"n\" is the total size of the training set.\n",
        "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        # \"eta*(lmbda/n)*w\" comes from the regularization term.\n",
        "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # backpropagate\n",
        "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "\n",
        "    def accuracy(self, data, convert=False):\n",
        "\n",
        "        if convert:\n",
        "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
        "        else:\n",
        "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
        "\n",
        "        result_accuracy = sum(int(x==y) for (x,y) in results)\n",
        "        return result_accuracy\n",
        "\n",
        "    def total_cost(self, data, lmbda, convert=False):\n",
        "        cost = 0.0\n",
        "        for x, y in data:\n",
        "            a = self.feedforward(x)\n",
        "            if convert: y = vectorized_result(y)\n",
        "            cost += self.cost.fn(a, y)/len(data)\n",
        "            cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
        "        return cost\n",
        "\n",
        "    # Save the neural network to the file \"filename\".\n",
        "    def save(self, filename):\n",
        "        data = {\"sizes\": self.sizes,\n",
        "                \"weights\": [w.tolist() for w in self.weights],\n",
        "                \"biases\": [b.tolist() for b in self.biases],\n",
        "                \"cost\": str(self.cost.__name__)}\n",
        "        f = open(filename, \"w\")\n",
        "        json.dump(data, f)\n",
        "        f.close()\n",
        "\n",
        "\n",
        "# Loading a Network\n",
        "def load(filename):\n",
        "\n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "\n",
        "# Miscellaneous functions\n",
        "def vectorized_result(j):\n",
        "\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is,\n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2024/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\".\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector\n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "\n",
        "    plt.imshow(training_inputs[1].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)"
      ],
      "metadata": {
        "id": "4KCSryRc-XA5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ],
      "metadata": {
        "id": "czAce4vR-Y_-",
        "outputId": "5fc8963e-2a5d-4475-d12a-69737475dc25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = Network2([784, 30, 10], cost=CrossEntropyCost)\n",
        "net2.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)"
      ],
      "metadata": {
        "id": "m3vhgcz_-bEl",
        "outputId": "3acc84e1-e684-4dab-8e02-4bccaf31da4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 3847.1386419861324\n",
            "Accuracy on training data: 47076 / 50000\n",
            "Cost on evaluation data: 3847.135380442634\n",
            "Accuracy on evaluation data: 9436 / 10000\n",
            "\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 4944.0503146501505\n",
            "Accuracy on training data: 47319 / 50000\n",
            "Cost on evaluation data: 4944.064328340706\n",
            "Accuracy on evaluation data: 9445 / 10000\n",
            "\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 5471.086312784078\n",
            "Accuracy on training data: 47808 / 50000\n",
            "Cost on evaluation data: 5471.10812963318\n",
            "Accuracy on evaluation data: 9549 / 10000\n",
            "\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 5828.233922737696\n",
            "Accuracy on training data: 47927 / 50000\n",
            "Cost on evaluation data: 5828.253675965366\n",
            "Accuracy on evaluation data: 9562 / 10000\n",
            "\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 5956.233010930484\n",
            "Accuracy on training data: 48028 / 50000\n",
            "Cost on evaluation data: 5956.262206303465\n",
            "Accuracy on evaluation data: 9550 / 10000\n",
            "\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 6141.555467863213\n",
            "Accuracy on training data: 48108 / 50000\n",
            "Cost on evaluation data: 6141.5845897745685\n",
            "Accuracy on evaluation data: 9563 / 10000\n",
            "\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 6217.7541911273165\n",
            "Accuracy on training data: 48271 / 50000\n",
            "Cost on evaluation data: 6217.791544641932\n",
            "Accuracy on evaluation data: 9607 / 10000\n",
            "\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 6332.810901803116\n",
            "Accuracy on training data: 48124 / 50000\n",
            "Cost on evaluation data: 6332.846512466892\n",
            "Accuracy on evaluation data: 9563 / 10000\n",
            "\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 6405.809073846333\n",
            "Accuracy on training data: 47985 / 50000\n",
            "Cost on evaluation data: 6405.851170121492\n",
            "Accuracy on evaluation data: 9515 / 10000\n",
            "\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 6408.038528726266\n",
            "Accuracy on training data: 48299 / 50000\n",
            "Cost on evaluation data: 6408.087301169329\n",
            "Accuracy on evaluation data: 9561 / 10000\n",
            "\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 6411.895911807246\n",
            "Accuracy on training data: 48310 / 50000\n",
            "Cost on evaluation data: 6411.9456379293315\n",
            "Accuracy on evaluation data: 9597 / 10000\n",
            "\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 6470.499802222179\n",
            "Accuracy on training data: 48331 / 50000\n",
            "Cost on evaluation data: 6470.538916347354\n",
            "Accuracy on evaluation data: 9612 / 10000\n",
            "\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 6434.35460710048\n",
            "Accuracy on training data: 48369 / 50000\n",
            "Cost on evaluation data: 6434.402368999419\n",
            "Accuracy on evaluation data: 9580 / 10000\n",
            "\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 6489.715814976821\n",
            "Accuracy on training data: 47803 / 50000\n",
            "Cost on evaluation data: 6489.756793205307\n",
            "Accuracy on evaluation data: 9513 / 10000\n",
            "\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 6504.44201152237\n",
            "Accuracy on training data: 48401 / 50000\n",
            "Cost on evaluation data: 6504.48851361505\n",
            "Accuracy on evaluation data: 9597 / 10000\n",
            "\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 6519.874359192147\n",
            "Accuracy on training data: 48353 / 50000\n",
            "Cost on evaluation data: 6519.921256032113\n",
            "Accuracy on evaluation data: 9599 / 10000\n",
            "\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 6528.653784486667\n",
            "Accuracy on training data: 48473 / 50000\n",
            "Cost on evaluation data: 6528.694257010503\n",
            "Accuracy on evaluation data: 9621 / 10000\n",
            "\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 6560.816271418615\n",
            "Accuracy on training data: 48156 / 50000\n",
            "Cost on evaluation data: 6560.862536079854\n",
            "Accuracy on evaluation data: 9538 / 10000\n",
            "\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 6550.22193144025\n",
            "Accuracy on training data: 48421 / 50000\n",
            "Cost on evaluation data: 6550.2602800956665\n",
            "Accuracy on evaluation data: 9613 / 10000\n",
            "\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 6571.164030070755\n",
            "Accuracy on training data: 48228 / 50000\n",
            "Cost on evaluation data: 6571.213781461186\n",
            "Accuracy on evaluation data: 9554 / 10000\n",
            "\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 6563.867205312353\n",
            "Accuracy on training data: 48247 / 50000\n",
            "Cost on evaluation data: 6563.913116124149\n",
            "Accuracy on evaluation data: 9566 / 10000\n",
            "\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 6552.124403394394\n",
            "Accuracy on training data: 48177 / 50000\n",
            "Cost on evaluation data: 6552.157087268531\n",
            "Accuracy on evaluation data: 9587 / 10000\n",
            "\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 6588.496536738649\n",
            "Accuracy on training data: 48282 / 50000\n",
            "Cost on evaluation data: 6588.534448932287\n",
            "Accuracy on evaluation data: 9590 / 10000\n",
            "\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 6588.004968348678\n",
            "Accuracy on training data: 48377 / 50000\n",
            "Cost on evaluation data: 6588.050003293003\n",
            "Accuracy on evaluation data: 9596 / 10000\n",
            "\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 6578.831116921807\n",
            "Accuracy on training data: 48379 / 50000\n",
            "Cost on evaluation data: 6578.877782704943\n",
            "Accuracy on evaluation data: 9587 / 10000\n",
            "\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 6569.232916541748\n",
            "Accuracy on training data: 48343 / 50000\n",
            "Cost on evaluation data: 6569.268577914716\n",
            "Accuracy on evaluation data: 9600 / 10000\n",
            "\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 6611.300423671341\n",
            "Accuracy on training data: 48422 / 50000\n",
            "Cost on evaluation data: 6611.349968372196\n",
            "Accuracy on evaluation data: 9589 / 10000\n",
            "\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 6632.201114132377\n",
            "Accuracy on training data: 48485 / 50000\n",
            "Cost on evaluation data: 6632.244085975814\n",
            "Accuracy on evaluation data: 9621 / 10000\n",
            "\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 6634.938306475407\n",
            "Accuracy on training data: 48377 / 50000\n",
            "Cost on evaluation data: 6634.990590525943\n",
            "Accuracy on evaluation data: 9575 / 10000\n",
            "\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 6641.6206812767505\n",
            "Accuracy on training data: 48255 / 50000\n",
            "Cost on evaluation data: 6641.669169601969\n",
            "Accuracy on evaluation data: 9580 / 10000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([3847.135380442634,\n",
              "  4944.064328340706,\n",
              "  5471.10812963318,\n",
              "  5828.253675965366,\n",
              "  5956.262206303465,\n",
              "  6141.5845897745685,\n",
              "  6217.791544641932,\n",
              "  6332.846512466892,\n",
              "  6405.851170121492,\n",
              "  6408.087301169329,\n",
              "  6411.9456379293315,\n",
              "  6470.538916347354,\n",
              "  6434.402368999419,\n",
              "  6489.756793205307,\n",
              "  6504.48851361505,\n",
              "  6519.921256032113,\n",
              "  6528.694257010503,\n",
              "  6560.862536079854,\n",
              "  6550.2602800956665,\n",
              "  6571.213781461186,\n",
              "  6563.913116124149,\n",
              "  6552.157087268531,\n",
              "  6588.534448932287,\n",
              "  6588.050003293003,\n",
              "  6578.877782704943,\n",
              "  6569.268577914716,\n",
              "  6611.349968372196,\n",
              "  6632.244085975814,\n",
              "  6634.990590525943,\n",
              "  6641.669169601969],\n",
              " [9436,\n",
              "  9445,\n",
              "  9549,\n",
              "  9562,\n",
              "  9550,\n",
              "  9563,\n",
              "  9607,\n",
              "  9563,\n",
              "  9515,\n",
              "  9561,\n",
              "  9597,\n",
              "  9612,\n",
              "  9580,\n",
              "  9513,\n",
              "  9597,\n",
              "  9599,\n",
              "  9621,\n",
              "  9538,\n",
              "  9613,\n",
              "  9554,\n",
              "  9566,\n",
              "  9587,\n",
              "  9590,\n",
              "  9596,\n",
              "  9587,\n",
              "  9600,\n",
              "  9589,\n",
              "  9621,\n",
              "  9575,\n",
              "  9580],\n",
              " [3847.1386419861324,\n",
              "  4944.0503146501505,\n",
              "  5471.086312784078,\n",
              "  5828.233922737696,\n",
              "  5956.233010930484,\n",
              "  6141.555467863213,\n",
              "  6217.7541911273165,\n",
              "  6332.810901803116,\n",
              "  6405.809073846333,\n",
              "  6408.038528726266,\n",
              "  6411.895911807246,\n",
              "  6470.499802222179,\n",
              "  6434.35460710048,\n",
              "  6489.715814976821,\n",
              "  6504.44201152237,\n",
              "  6519.874359192147,\n",
              "  6528.653784486667,\n",
              "  6560.816271418615,\n",
              "  6550.22193144025,\n",
              "  6571.164030070755,\n",
              "  6563.867205312353,\n",
              "  6552.124403394394,\n",
              "  6588.496536738649,\n",
              "  6588.004968348678,\n",
              "  6578.831116921807,\n",
              "  6569.232916541748,\n",
              "  6611.300423671341,\n",
              "  6632.201114132377,\n",
              "  6634.938306475407,\n",
              "  6641.6206812767505],\n",
              " [47076,\n",
              "  47319,\n",
              "  47808,\n",
              "  47927,\n",
              "  48028,\n",
              "  48108,\n",
              "  48271,\n",
              "  48124,\n",
              "  47985,\n",
              "  48299,\n",
              "  48310,\n",
              "  48331,\n",
              "  48369,\n",
              "  47803,\n",
              "  48401,\n",
              "  48353,\n",
              "  48473,\n",
              "  48156,\n",
              "  48421,\n",
              "  48228,\n",
              "  48247,\n",
              "  48177,\n",
              "  48282,\n",
              "  48377,\n",
              "  48379,\n",
              "  48343,\n",
              "  48422,\n",
              "  48485,\n",
              "  48377,\n",
              "  48255])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()\n",
        "netQuadratic = Network2([784, 30, 10], cost=QuadraticCost)\n",
        "netQuadratic.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)\n"
      ],
      "metadata": {
        "id": "JlPtN5Wa-fed",
        "outputId": "3617a783-8934-48a4-f18c-c5e127263ed6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 988.6405376082955\n",
            "Accuracy on training data: 45909 / 50000\n",
            "Cost on evaluation data: 988.6339511902452\n",
            "Accuracy on evaluation data: 9257 / 10000\n",
            "\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 1196.0778769581034\n",
            "Accuracy on training data: 46473 / 50000\n",
            "Cost on evaluation data: 1196.0730261931608\n",
            "Accuracy on evaluation data: 9341 / 10000\n",
            "\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 1297.4586188053784\n",
            "Accuracy on training data: 46969 / 50000\n",
            "Cost on evaluation data: 1297.4552192298222\n",
            "Accuracy on evaluation data: 9439 / 10000\n",
            "\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 1358.8998142945268\n",
            "Accuracy on training data: 47054 / 50000\n",
            "Cost on evaluation data: 1358.8969812181645\n",
            "Accuracy on evaluation data: 9428 / 10000\n",
            "\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 1394.4147782718903\n",
            "Accuracy on training data: 47204 / 50000\n",
            "Cost on evaluation data: 1394.4124876070537\n",
            "Accuracy on evaluation data: 9473 / 10000\n",
            "\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 1422.4160044554021\n",
            "Accuracy on training data: 47240 / 50000\n",
            "Cost on evaluation data: 1422.4136618609584\n",
            "Accuracy on evaluation data: 9476 / 10000\n",
            "\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 1441.7948276649754\n",
            "Accuracy on training data: 47390 / 50000\n",
            "Cost on evaluation data: 1441.7927689867447\n",
            "Accuracy on evaluation data: 9493 / 10000\n",
            "\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 1457.4127691995598\n",
            "Accuracy on training data: 47465 / 50000\n",
            "Cost on evaluation data: 1457.4112763453659\n",
            "Accuracy on evaluation data: 9516 / 10000\n",
            "\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 1468.5259611233091\n",
            "Accuracy on training data: 47505 / 50000\n",
            "Cost on evaluation data: 1468.5243193287954\n",
            "Accuracy on evaluation data: 9533 / 10000\n",
            "\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 1480.8008333290961\n",
            "Accuracy on training data: 47591 / 50000\n",
            "Cost on evaluation data: 1480.7992911306212\n",
            "Accuracy on evaluation data: 9551 / 10000\n",
            "\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 1489.5861524617728\n",
            "Accuracy on training data: 47664 / 50000\n",
            "Cost on evaluation data: 1489.5852651026673\n",
            "Accuracy on evaluation data: 9537 / 10000\n",
            "\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 1496.8870546711123\n",
            "Accuracy on training data: 47551 / 50000\n",
            "Cost on evaluation data: 1496.8861600005412\n",
            "Accuracy on evaluation data: 9514 / 10000\n",
            "\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 1502.3481129195065\n",
            "Accuracy on training data: 47671 / 50000\n",
            "Cost on evaluation data: 1502.3467591697017\n",
            "Accuracy on evaluation data: 9553 / 10000\n",
            "\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 1510.9501480328236\n",
            "Accuracy on training data: 47716 / 50000\n",
            "Cost on evaluation data: 1510.9492860903135\n",
            "Accuracy on evaluation data: 9543 / 10000\n",
            "\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 1514.179432264404\n",
            "Accuracy on training data: 47701 / 50000\n",
            "Cost on evaluation data: 1514.178795796753\n",
            "Accuracy on evaluation data: 9554 / 10000\n",
            "\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 1521.4605262485318\n",
            "Accuracy on training data: 47735 / 50000\n",
            "Cost on evaluation data: 1521.45953509964\n",
            "Accuracy on evaluation data: 9558 / 10000\n",
            "\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 1523.3318956143778\n",
            "Accuracy on training data: 47719 / 50000\n",
            "Cost on evaluation data: 1523.330581661284\n",
            "Accuracy on evaluation data: 9563 / 10000\n",
            "\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 1528.6913871038673\n",
            "Accuracy on training data: 47824 / 50000\n",
            "Cost on evaluation data: 1528.6907991680819\n",
            "Accuracy on evaluation data: 9576 / 10000\n",
            "\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 1533.1965070467543\n",
            "Accuracy on training data: 47731 / 50000\n",
            "Cost on evaluation data: 1533.1954684048785\n",
            "Accuracy on evaluation data: 9556 / 10000\n",
            "\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 1536.255972300068\n",
            "Accuracy on training data: 47808 / 50000\n",
            "Cost on evaluation data: 1536.2554795013566\n",
            "Accuracy on evaluation data: 9560 / 10000\n",
            "\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 1538.6907171474081\n",
            "Accuracy on training data: 47802 / 50000\n",
            "Cost on evaluation data: 1538.6901785616037\n",
            "Accuracy on evaluation data: 9572 / 10000\n",
            "\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 1542.1759994842928\n",
            "Accuracy on training data: 47829 / 50000\n",
            "Cost on evaluation data: 1542.1751590780711\n",
            "Accuracy on evaluation data: 9566 / 10000\n",
            "\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 1542.675411540564\n",
            "Accuracy on training data: 47905 / 50000\n",
            "Cost on evaluation data: 1542.6748192679356\n",
            "Accuracy on evaluation data: 9582 / 10000\n",
            "\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 1544.3213140352354\n",
            "Accuracy on training data: 47830 / 50000\n",
            "Cost on evaluation data: 1544.3206858026094\n",
            "Accuracy on evaluation data: 9581 / 10000\n",
            "\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 1548.3673277370283\n",
            "Accuracy on training data: 47910 / 50000\n",
            "Cost on evaluation data: 1548.3669353421196\n",
            "Accuracy on evaluation data: 9580 / 10000\n",
            "\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 1549.6946428493827\n",
            "Accuracy on training data: 47881 / 50000\n",
            "Cost on evaluation data: 1549.694375949792\n",
            "Accuracy on evaluation data: 9574 / 10000\n",
            "\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 1551.615498816418\n",
            "Accuracy on training data: 47936 / 50000\n",
            "Cost on evaluation data: 1551.6150375334557\n",
            "Accuracy on evaluation data: 9581 / 10000\n",
            "\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 1554.314635258946\n",
            "Accuracy on training data: 47967 / 50000\n",
            "Cost on evaluation data: 1554.3141796522189\n",
            "Accuracy on evaluation data: 9598 / 10000\n",
            "\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 1557.4865706734765\n",
            "Accuracy on training data: 47940 / 50000\n",
            "Cost on evaluation data: 1557.486542191132\n",
            "Accuracy on evaluation data: 9575 / 10000\n",
            "\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 1556.5621892164813\n",
            "Accuracy on training data: 47964 / 50000\n",
            "Cost on evaluation data: 1556.5618733138333\n",
            "Accuracy on evaluation data: 9587 / 10000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([988.6339511902452,\n",
              "  1196.0730261931608,\n",
              "  1297.4552192298222,\n",
              "  1358.8969812181645,\n",
              "  1394.4124876070537,\n",
              "  1422.4136618609584,\n",
              "  1441.7927689867447,\n",
              "  1457.4112763453659,\n",
              "  1468.5243193287954,\n",
              "  1480.7992911306212,\n",
              "  1489.5852651026673,\n",
              "  1496.8861600005412,\n",
              "  1502.3467591697017,\n",
              "  1510.9492860903135,\n",
              "  1514.178795796753,\n",
              "  1521.45953509964,\n",
              "  1523.330581661284,\n",
              "  1528.6907991680819,\n",
              "  1533.1954684048785,\n",
              "  1536.2554795013566,\n",
              "  1538.6901785616037,\n",
              "  1542.1751590780711,\n",
              "  1542.6748192679356,\n",
              "  1544.3206858026094,\n",
              "  1548.3669353421196,\n",
              "  1549.694375949792,\n",
              "  1551.6150375334557,\n",
              "  1554.3141796522189,\n",
              "  1557.486542191132,\n",
              "  1556.5618733138333],\n",
              " [9257,\n",
              "  9341,\n",
              "  9439,\n",
              "  9428,\n",
              "  9473,\n",
              "  9476,\n",
              "  9493,\n",
              "  9516,\n",
              "  9533,\n",
              "  9551,\n",
              "  9537,\n",
              "  9514,\n",
              "  9553,\n",
              "  9543,\n",
              "  9554,\n",
              "  9558,\n",
              "  9563,\n",
              "  9576,\n",
              "  9556,\n",
              "  9560,\n",
              "  9572,\n",
              "  9566,\n",
              "  9582,\n",
              "  9581,\n",
              "  9580,\n",
              "  9574,\n",
              "  9581,\n",
              "  9598,\n",
              "  9575,\n",
              "  9587],\n",
              " [988.6405376082955,\n",
              "  1196.0778769581034,\n",
              "  1297.4586188053784,\n",
              "  1358.8998142945268,\n",
              "  1394.4147782718903,\n",
              "  1422.4160044554021,\n",
              "  1441.7948276649754,\n",
              "  1457.4127691995598,\n",
              "  1468.5259611233091,\n",
              "  1480.8008333290961,\n",
              "  1489.5861524617728,\n",
              "  1496.8870546711123,\n",
              "  1502.3481129195065,\n",
              "  1510.9501480328236,\n",
              "  1514.179432264404,\n",
              "  1521.4605262485318,\n",
              "  1523.3318956143778,\n",
              "  1528.6913871038673,\n",
              "  1533.1965070467543,\n",
              "  1536.255972300068,\n",
              "  1538.6907171474081,\n",
              "  1542.1759994842928,\n",
              "  1542.675411540564,\n",
              "  1544.3213140352354,\n",
              "  1548.3673277370283,\n",
              "  1549.6946428493827,\n",
              "  1551.615498816418,\n",
              "  1554.314635258946,\n",
              "  1557.4865706734765,\n",
              "  1556.5621892164813],\n",
              " [45909,\n",
              "  46473,\n",
              "  46969,\n",
              "  47054,\n",
              "  47204,\n",
              "  47240,\n",
              "  47390,\n",
              "  47465,\n",
              "  47505,\n",
              "  47591,\n",
              "  47664,\n",
              "  47551,\n",
              "  47671,\n",
              "  47716,\n",
              "  47701,\n",
              "  47735,\n",
              "  47719,\n",
              "  47824,\n",
              "  47731,\n",
              "  47808,\n",
              "  47802,\n",
              "  47829,\n",
              "  47905,\n",
              "  47830,\n",
              "  47910,\n",
              "  47881,\n",
              "  47936,\n",
              "  47967,\n",
              "  47940,\n",
              "  47964])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "MNielsen_network.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}