{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidGoing/PHYS3151-Machine-Learning-in-Physics-2024/blob/main/logistic-regression/Grading_weight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Layer"
      ],
      "metadata": {
        "id": "cDU-rIlaZqzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the grading method of this course.  Four assignments account for in total 30%, a project and a presentation each takes up 20%, and a final exam accounts for 30%.\n",
        "Consider a (fictitious) grading cut: F(<50), D(50\\~60), C(60\\~75), B(75\\~90) A(90\\~100), without any subdivision. Assign an integer from 0 to 4 to each of them, with 0 being F and 4 being A."
      ],
      "metadata": {
        "id": "PPMUgOX2ZqzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import csv"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "HbAYw2YxZqzD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "letter_grade=[\"F\",\"D\",\"C\",\"B\",\"A\"]\n",
        "weight=[0.075, 0.075, 0.075, 0.075, 0.2, 0.2, 0.3]\n",
        "np.random.seed\n",
        "with open('grade_data.csv', mode='w') as sample_file:\n",
        "    grade_writer = csv.writer(sample_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    grade_writer.writerow(['HW1','HW2','HW3','HW4','Project','Presentation','Exam','Grade',])\n",
        "    for i in range (0, 1000):\n",
        "        mark_temp=np.random.normal(60, 60, 7)\n",
        "        #prevent marks from going out of the range [0,100]\n",
        "        mark_temp=[max(p,0) for p in mark_temp]\n",
        "        mark_temp=[min(p,100) for p in mark_temp]\n",
        "        mark_weighted=np.dot(weight,mark_temp)\n",
        "        grade=0\n",
        "        if mark_weighted>=50:\n",
        "          grade+=1\n",
        "          if mark_weighted>=60:\n",
        "            grade+=1\n",
        "            if mark_weighted>=75:\n",
        "              grade+=1\n",
        "              if mark_weighted>=90:\n",
        "                grade+=1\n",
        "        mark_temp.append(grade)\n",
        "        grade_writer.writerow(mark_temp)"
      ],
      "metadata": {
        "id": "dDgHkoqD3SG9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/grade_data.csv')\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3BpiAdgVZx2",
        "outputId": "5969b5a7-61dc-4c05-d899-0fc4a8cd2bb1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            HW1         HW2         HW3         HW4     Project  Presentation  \\\n",
            "0    100.000000  100.000000   30.954987    6.821568   13.376814      0.000000   \n",
            "1     49.797146   46.227928   56.773286    0.000000   32.709680     74.314485   \n",
            "2     20.366042    7.352175   87.481686   61.355763  100.000000      0.000000   \n",
            "3     50.744332   66.416793   10.791449   86.776878   30.667904    100.000000   \n",
            "4     10.557723   94.261697    0.000000   84.479583   58.941993      0.000000   \n",
            "..          ...         ...         ...         ...         ...           ...   \n",
            "995   94.179794    0.000000   55.672433  100.000000   26.116161     41.534595   \n",
            "996    0.000000    0.000000  100.000000   14.663096  100.000000    100.000000   \n",
            "997   32.590867   33.482063  100.000000   77.136192   67.067924     97.539828   \n",
            "998  100.000000    0.000000   17.654241   73.715375   77.781429      0.000000   \n",
            "999    0.000000   25.242776    0.000000   49.191349   28.109211     59.636227   \n",
            "\n",
            "           Exam  Grade  \n",
            "0    100.000000      1  \n",
            "1      0.000000      0  \n",
            "2     16.207467      0  \n",
            "3    100.000000      2  \n",
            "4      3.311797      0  \n",
            "..          ...    ...  \n",
            "995  100.000000      2  \n",
            "996    0.000000      0  \n",
            "997   29.442934      1  \n",
            "998   98.641772      1  \n",
            "999    0.000000      0  \n",
            "\n",
            "[1000 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Layer"
      ],
      "metadata": {
        "id": "pJnmu4UZZqzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the representation of the hypothesis, we define <span style=\"border-bottom: dashed\">sigmoid function</span> :\n",
        "$$\n",
        "h_{\\theta}=\\frac{1}{1+e^{-\\theta^Tx}}\n",
        "$$\n",
        "Note that $\\theta^Tx$ can be non-linear.\n",
        "    <br> <span style=\"border-bottom: dashed\">Cost function</span> in logistic refression is defined as:\n",
        "$$\n",
        "J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^m(y\\log(h_\\theta)+(1-y)\\log(1-h_\\theta)),\n",
        "$$\n",
        "and still, we can use gradient descent to minimize it.\n",
        "    \n",
        "<br>In addition, we use <span style=\"border-bottom: dashed\">accuracy function</span> to see how well the algorithm works, which is defined as:\n",
        "$$\n",
        "Accuracy=\\frac{1}{m}\\sum_{i=1}^{m}\\delta_{predicted,real}\n",
        "$$"
      ],
      "metadata": {
        "id": "rFmoJRWoZqzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The N-dimensional hyperplane that satisfies $z=\\theta^{T}x=0$ is called the decision boundary.  Ideally, it separates two classes of samples.\n",
        "\n",
        "$z\\gt0$ implies the sample is more likely to belong to class $y=1$, and vise versa"
      ],
      "metadata": {
        "id": "87QXmoVo4x47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <img title=\"decision boundary\" src=\"https://github.com/LeoisWTT/PHYS3151-Machine-Learning-in-Physics-2023/blob/main/logistic-regression/Decision_boundary.png?raw=1\" width=\"800\" /> <br>"
      ],
      "metadata": {
        "id": "V1G9VagSlIxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import fmin_tnc\n",
        "\n",
        "\n",
        "class LogisticRegressionUsingGD:\n",
        "\n",
        "# Activation function used to map any real value between 0 and 1\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Computes the weighted sum of inputs Similar to Linear Regression\n",
        "    @staticmethod\n",
        "    def net_input(theta, x):\n",
        "        return np.dot(x, theta)\n",
        "\n",
        "# Calculates the probability that an instance belongs to a particular class\n",
        "    def probability(self, theta, x):\n",
        "        return self.sigmoid(self.net_input(theta, x))\n",
        "\n",
        "# Computes the cost function for all the training samples\n",
        "    def cost_function(self, theta, x, y):\n",
        "        m = x.shape[0]\n",
        "        total_cost = -(1 / m) * np.sum([np.log(self.probability(theta, x[i])) if y[i]==1 else np.log(1-self.probability(theta, x[i])) for i in range(m)])\n",
        "        return total_cost\n",
        "\n",
        "# Computes the gradient of the cost function at the point theta\n",
        "    def gradient(self, theta, x, y):\n",
        "        m = x.shape[0]\n",
        "        return (1 / m) * np.dot(x.T, self.sigmoid(self.net_input(theta, x)) - y)\n",
        "\n",
        "    def fit(self, x, y, theta):\n",
        "        opt_weights = fmin_tnc(func=self.cost_function, x0=theta, fprime=self.gradient, args=(x, y.flatten()))\n",
        "        self.w_ = opt_weights[0]\n",
        "        return self\n",
        "\n",
        "    def predict(self, x):\n",
        "        theta = self.w_[:, np.newaxis]\n",
        "        return self.probability(theta, x)\n",
        "\n",
        "    def accuracy(self, x, actual_classes, probab_threshold=0.5):\n",
        "        predicted_classes = (self.predict(x) >= probab_threshold).astype(int)\n",
        "        predicted_classes = predicted_classes.flatten()\n",
        "        accuracy = np.mean(predicted_classes == actual_classes)\n",
        "        return accuracy * 100"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "mCkza3L7ZqzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we pick the features and outcome out of the initial data."
      ],
      "metadata": {
        "id": "x-mbv2xXZqzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = df\n",
        "X = data.iloc[:, :7]        #features\n",
        "grade = data.iloc[:, 7]         #outcome\n",
        "X = np.c_[np.ones((X.shape[0], 1)), X]    #we need x_0"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "_KU7RTmqZqzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input whether or not each sample is in or above B range.  Assign 0 to all of those below B range, and vice versa. This devides the sample into two different classes for us to perform logistic regression.\n",
        "\n",
        "As a result, we should be able to find the boundary between grade B and C."
      ],
      "metadata": {
        "id": "a5N_0sa0moMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <img title=\"grade\" src=\"https://github.com/LeoisWTT/PHYS3151-Machine-Learning-in-Physics-2023/blob/main/logistic-regression/grade.png?raw=1\" width=\"800\" /> <br>"
      ],
      "metadata": {
        "id": "mA1Ry7cJlK2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grade_here=3\n",
        "y = [g>=grade_here for g in grade]                 #devide sample by whether they are in or above B(3) range\n",
        "y = np.array(y)\n",
        "theta = np.zeros((X.shape[1], 1))"
      ],
      "metadata": {
        "id": "c0yOEN7MlrZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Logistic Regression algorithm."
      ],
      "metadata": {
        "id": "t9oxGFVXZqzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = LogisticRegressionUsingGD()\n",
        "model1.fit(X, y, theta)\n",
        "accuracy = model1.accuracy(X, y.flatten())\n",
        "parameters = model1.w_\n",
        "print(\"The accuracy of the model is {}\".format(accuracy))\n",
        "print(\"The model parameters got by Gradient descent:\")\n",
        "print(parameters)"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "tTiuBiMYZqzJ",
        "outputId": "7b1ce54d-4046-433a-fe7a-b9f12d77e7a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the model is 100.0\n",
            "The model parameters got by Gradient descent:\n",
            "[-1.04302438e+03  1.03197740e+00  1.06828718e+00  1.03913938e+00\n",
            "  1.04336035e+00  2.77918227e+00  2.80358328e+00  4.14899099e+00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-349e78460cf8>:10: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We normalize the parameters ($\\theta$) by letting the sum of weight (excluding bias term) equal to 1, as it should be.  We can see the 7 weights we used to determine the grade of each sample and bias term being negative of cut line."
      ],
      "metadata": {
        "id": "AMr8k5gseecn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_normalized=parameters/sum(parameters[1:])\n",
        "print(\"The cut line for\", letter_grade[grade_here], \"range is {}\".format(-parameters_normalized[0]))\n",
        "print(\"The weights of the 7 components are {}\".format(parameters_normalized[1:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fMEQqB4dm9B",
        "outputId": "50931e3b-17ae-45f2-f842-c773ccc680b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cut line for B range is 74.95941787282493\n",
            "The weights of the 7 components are [0.0741655  0.07677499 0.07468021 0.07498356 0.19973252 0.20148615\n",
            " 0.29817707]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try the same by deviding in C range and reach the same result, except for the bias term"
      ],
      "metadata": {
        "id": "xRg05s9HhrVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grade_here=2\n",
        "y = [g>=grade_here for g in grade]                 #devide sample by whether they are in or above C(2) range\n",
        "y = np.array(y)\n",
        "theta = np.zeros((X.shape[1], 1))"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "PM5rbmBahp88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = LogisticRegressionUsingGD()\n",
        "model2.fit(X, y, theta)\n",
        "accuracy = model2.accuracy(X, y.flatten())\n",
        "parameters = model2.w_\n",
        "print(\"The accuracy of the model is {}\".format(accuracy))\n",
        "print(\"The model parameters got by Gradient descent:\")\n",
        "print(parameters)"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "2ed52fd4-6754-494d-a13a-8112bc70149d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqEaNYDwhp89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-349e78460cf8>:10: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the model is 100.0\n",
            "The model parameters got by Gradient descent:\n",
            "[-1320.86557045     1.65564797     1.66119478     1.53581502\n",
            "     1.56276266     4.46217408     4.4774591      6.65694482]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_normalized=parameters/sum(parameters[1:])\n",
        "print(\"The cut line for\", letter_grade[grade_here], \"range is {}\".format(-parameters_normalized[0]))\n",
        "print(\"The weights of the 7 components are {}\".format(parameters_normalized[1:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f7517a4-f546-4c25-ec48-ee7185c7ff26",
        "id": "qXQHs3ryljmb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cut line for C range is 60.00661753760554\n",
            "The weights of the 7 components are [0.0752157  0.07546769 0.06977172 0.07099595 0.20271554 0.20340993\n",
            " 0.30242346]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the case where we cannot know about all relevant inputs. Is it still possible to get the result using remaining data.\n",
        "\n",
        "In the following case we ignore the effect of the first two assignments which takes up 15% combined."
      ],
      "metadata": {
        "id": "JXZ2-NWZxGg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X2 = data.iloc[:, 2:7]        #features (excluding first 2 columns)\n",
        "grade = data.iloc[:, 7]         #outcome\n",
        "X2 = np.c_[np.ones((X2.shape[0], 1)), X2]    #we need x_0"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "nErzDFcVw72e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grade_here=3\n",
        "y = [g>=grade_here for g in grade]                 #devide sample by whether they are in or above B(3) range\n",
        "y = np.array(y)\n",
        "theta = np.zeros((X2.shape[1], 1))"
      ],
      "metadata": {
        "id": "ioNtJtOmw72f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = LogisticRegressionUsingGD()\n",
        "model3.fit(X2, y, theta)\n",
        "accuracy = model3.accuracy(X2, y.flatten())\n",
        "parameters = model3.w_\n",
        "print(\"The accuracy of the model is {}\".format(accuracy))\n",
        "print(\"The model parameters got by Gradient descent:\")\n",
        "print(parameters)"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "162609cf-8b40-4e2e-b932-fb2c3c8c385b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMVDWHZ7w72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the model is 95.39999999999999\n",
            "The model parameters got by Gradient descent:\n",
            "[-31.04156189   0.03454377   0.03704007   0.09284211   0.09181337\n",
            "   0.14214254]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time we normalize the weights to sum up to only 85%, but rescale the bias term up so that it is still out of 100 points.\n",
        "\n",
        "Note that we can still get a result for the remaining components with slightly less accuracy."
      ],
      "metadata": {
        "id": "oDR-naEUxyc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_normalized=parameters/sum(parameters[1:])*0.85\n",
        "print(\"The cut line for\", letter_grade[grade_here], \"range is {}\".format(-parameters_normalized[0]/0.85))\n",
        "print(\"The weights of the remaining 5 components are {}\".format(parameters_normalized[1:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9634e1-aeeb-475d-fb81-9b34d201ac46",
        "id": "trDzfh9tw72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cut line for B range is 77.91911508824822\n",
            "The weights of the remaining 5 components are [0.07370366 0.07902985 0.19809083 0.19589588 0.30327978]\n"
          ]
        }
      ]
    }
  ]
}